{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model for Reddit, Twitter, and a combined dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string, csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%run data_proc.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "loaddata() missing 3 required positional arguments: 'word_file_path', 'split_word_path', and 'emoji_file_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-c784fc464bb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mreddit_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../data/reddit/train-balanced-sarcasm.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtwitter_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../data/Twitter/train/Train_v1.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtwitter_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaddata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtwitter_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtwitter_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: loaddata() missing 3 required positional arguments: 'word_file_path', 'split_word_path', and 'emoji_file_path'"
     ]
    }
   ],
   "source": [
    "# Reddit dataset\n",
    "REDDIT_TRAIN= '../data/reddit/train-balanced-sarcasm.csv'\n",
    "\n",
    "# Twitter dataset\n",
    "TWITTER_TRAIN = '../data/Twitter/train/Train_v1.txt'\n",
    "EMOJI_DICT = '../data/Twitter/emoji_unicode_names_final.txt'\n",
    "WORD_LIST = '../data/Twitter/word_list.txt'\n",
    "WORD_SPLIT = '../data/Twitter/word_split.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df = loaddata(twitter_train)\n",
    "with open(twitter_train, 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    data = list(reader)\n",
    "    df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                                                        0  \\\n",
      "0      TrainSen\\t0\\t@0430yes i hope youre lurking rn....   \n",
      "1      TrainSen\\t0\\t05 really taught me a valuable le...   \n",
      "2      TrainSen\\t0\\t@098BERRY Never had a voice to pr...   \n",
      "3      TrainSen\\t0\\t@0hMySt4rs Rest in peace & love t...   \n",
      "4      TrainSen\\t0\\t100 days until Christmas! ðŸŒ² #too ...   \n",
      "5      TrainSen\\t0\\t@100_ThingsILove @WhatNinaSpotted...   \n",
      "6      TrainSen\\t0\\t100 words short of the word requi...   \n",
      "7      TrainSen\\t0\\t@1010xlhacker it was nice hanging...   \n",
      "8      TrainSen\\t0\\t10k walk this morning. We did an ...   \n",
      "9      TrainSen\\t0\\t10 minutes to eat #challengeaccepted   \n",
      "10     TrainSen\\t0\\t10 Times One Direction Had the Pe...   \n",
      "11     TrainSen\\t0\\t#10TurnOns Yet again if you list ...   \n",
      "12     TrainSen\\t0\\t@11aawwgg021 wow! sweet butt for ...   \n",
      "13           TrainSen\\t0\\t11 o'clock cant come no faster   \n",
      "14     TrainSen\\t0\\t11pm on a Saturday night and Im i...   \n",
      "15              TrainSen\\t0\\t12:30 can't come any faster   \n",
      "16                                      TrainSen\\t0\\t$13   \n",
      "17     TrainSen\\t0\\t15-year-old boy fighting for his ...   \n",
      "18     TrainSen\\t0\\t17 years ago today we made one of...   \n",
      "19     TrainSen\\t0\\t180 days until summer! #notthatba...   \n",
      "20     TrainSen\\t0\\t1966 was when In Cold Blood scare...   \n",
      "21     TrainSen\\t0\\t19 Million Project brings journal...   \n",
      "22     TrainSen\\t0\\t1 am and I can already tell today...   \n",
      "23     TrainSen\\t0\\t@1dChicas_ @kingjennny first reac...   \n",
      "24     TrainSen\\t0\\t1D family! We want to see all of ...   \n",
      "25     TrainSen\\t0\\t1D give boring performances it's ...   \n",
      "26     TrainSen\\t0\\t#1DonJonathanRoss i have never la...   \n",
      "27     TrainSen\\t0\\t#1DonJonathanRoss IVE NEVER LAUGH...   \n",
      "28     TrainSen\\t0\\t1d the kings of never promoting y...   \n",
      "29     TrainSen\\t0\\t1) Good Grades 2) Social Life 3) ...   \n",
      "...                                                  ...   \n",
      "39750  TrainSen\\t1\\t@YouTube I don't understand the n...   \n",
      "39751  TrainSen\\t1\\t@YouTube If your platforms dosen'...   \n",
      "39752  TrainSen\\t1\\t@YouTube I would be fucking homel...   \n",
      "39753  TrainSen\\t1\\t@YouTube Just finished watching \"...   \n",
      "39754  TrainSen\\t1\\t@YouTube @soartuck i fucc wit u f...   \n",
      "39755  TrainSen\\t1\\t@YouTube thanks for ruining the b...   \n",
      "39756  TrainSen\\t1\\t@YouTube That or you'll realize h...   \n",
      "39757  TrainSen\\t1\\t@YouTube there's free porn you ca...   \n",
      "39758                TrainSen\\t1\\tYou've changed \"... No   \n",
      "39759    TrainSen\\t1\\tYou've got higher grades than mine   \n",
      "39760  TrainSen\\t1\\tYou've got roughly. 003 seconds a...   \n",
      "39761                TrainSen\\t1\\tYou wanna be tied down   \n",
      "39762  TrainSen\\t1\\tYou wanna see a perfect relations...   \n",
      "39763  TrainSen\\t1\\tyou wear that shirt a lot \" yes b...   \n",
      "39764  TrainSen\\t1\\tyou were my cup of tea but I drin...   \n",
      "39765  TrainSen\\t1\\tYou wish your were the topic... #...   \n",
      "39766                                   TrainSen\\t1\\tYum   \n",
      "39767  TrainSen\\t1\\t@yumizukiluna i'd have to find so...   \n",
      "39768  TrainSen\\t1\\t@yungdeercher well the parent was...   \n",
      "39769  TrainSen\\t1\\tYup. Don't I just feel awesome. #...   \n",
      "39770                                   TrainSen\\t1\\tYup   \n",
      "39771  TrainSen\\t1\\t@yusef_ram tpab has no reply valu...   \n",
      "39772  TrainSen\\t1\\t@zackalltimelow sometimes I wish ...   \n",
      "39773           TrainSen\\t1\\t@ZaidZamanHamid #Not at all   \n",
      "39774  TrainSen\\t1\\t@zaynmalik go and translate the t...   \n",
      "39775  TrainSen\\t1\\t@Zendaya I could see the makeup a...   \n",
      "39776  TrainSen\\t1\\t@ZiggiWatkins11 Slvr... That's gr...   \n",
      "39777  TrainSen\\t1\\t@zoso4986 @Nero He is the fag we ...   \n",
      "39778  TrainSen\\t1\\tZuma sounding like Kanye West rig...   \n",
      "39779  TrainSen\\t1\\t@ZZUCRU @UWDawgPack So true. Stud...   \n",
      "\n",
      "                                                       1  \\\n",
      "0                                  pretty please?! ðŸ˜­ ðŸ˜­ ðŸ˜­   \n",
      "1                                                   None   \n",
      "2       so you fed me shit to digest. I wish I had a ...   \n",
      "3                                                   None   \n",
      "4                                                   None   \n",
      "5                                                   None   \n",
      "6                 I'm going to bed. #rebel #ihatehistory   \n",
      "7                                                   None   \n",
      "8                                                   None   \n",
      "9                                                   None   \n",
      "10                                                  None   \n",
      "11                                                  None   \n",
      "12                                                  None   \n",
      "13                                                  None   \n",
      "14                                                  None   \n",
      "15                                                  None   \n",
      "16      180 invested in our mission on #GiveMiamiDay!...   \n",
      "17                                                  None   \n",
      "18                                                  None   \n",
      "19                                                  None   \n",
      "20      Bruno Sammartino slammed us & John Lennon sho...   \n",
      "21      coders and humanitarians together around the ...   \n",
      "22                                                  None   \n",
      "23                                                  None   \n",
      "24                         show us using #1DUltimateArt!   \n",
      "25                                                  None   \n",
      "26                                                  None   \n",
      "27                                                  None   \n",
      "28                                                  None   \n",
      "29                                                  None   \n",
      "...                                                  ...   \n",
      "39750                                               None   \n",
      "39751   everybody should leave you behind a D find so...   \n",
      "39752                                               None   \n",
      "39753                                               None   \n",
      "39754                                               None   \n",
      "39755      I guess I won't make $ on this tweet #sarcasm   \n",
      "39756                                               None   \n",
      "39757                                               None   \n",
      "39758   I think the proper term is \" I've stopped try...   \n",
      "39759                 not because you're smarter than me   \n",
      "39760                                               None   \n",
      "39761                   always have to answer to someone   \n",
      "39762                                               None   \n",
      "39763                                               None   \n",
      "39764                                               None   \n",
      "39765                                               None   \n",
      "39766                  I love toilet water #not #sarcasm   \n",
      "39767                                               None   \n",
      "39768   but barella was upset I didn't handle it like...   \n",
      "39769                                               None   \n",
      "39770   I'm a jumper if i want it im goin for it.. #N...   \n",
      "39771                                               None   \n",
      "39772                                               None   \n",
      "39773     if somebody says that he is an #Idiot #sarcasm   \n",
      "39774                                               None   \n",
      "39775                                               None   \n",
      "39776                                               None   \n",
      "39777                                               None   \n",
      "39778                                               None   \n",
      "39779                                               None   \n",
      "\n",
      "                                                       2     3     4     5  \\\n",
      "0                                                   None  None  None  None   \n",
      "1                                                   None  None  None  None   \n",
      "2                              my flaws are open season.  None  None  None   \n",
      "3                                                   None  None  None  None   \n",
      "4                                                   None  None  None  None   \n",
      "5                                                   None  None  None  None   \n",
      "6                                                   None  None  None  None   \n",
      "7                                                   None  None  None  None   \n",
      "8                                                   None  None  None  None   \n",
      "9                                                   None  None  None  None   \n",
      "10                                                  None  None  None  None   \n",
      "11                                                  None  None  None  None   \n",
      "12                                                  None  None  None  None   \n",
      "13                                                  None  None  None  None   \n",
      "14                                                  None  None  None  None   \n",
      "15                                                  None  None  None  None   \n",
      "16                                                  None  None  None  None   \n",
      "17                                                  None  None  None  None   \n",
      "18                                                  None  None  None  None   \n",
      "19                                                  None  None  None  None   \n",
      "20                                                  None  None  None  None   \n",
      "21                                                  None  None  None  None   \n",
      "22                                                  None  None  None  None   \n",
      "23                                                  None  None  None  None   \n",
      "24                                                  None  None  None  None   \n",
      "25                                                  None  None  None  None   \n",
      "26                                                  None  None  None  None   \n",
      "27                                                  None  None  None  None   \n",
      "28                                                  None  None  None  None   \n",
      "29                                                  None  None  None  None   \n",
      "...                                                  ...   ...   ...   ...   \n",
      "39750                                               None  None  None  None   \n",
      "39751                                               None  None  None  None   \n",
      "39752                                               None  None  None  None   \n",
      "39753                                               None  None  None  None   \n",
      "39754                                               None  None  None  None   \n",
      "39755                                               None  None  None  None   \n",
      "39756                                               None  None  None  None   \n",
      "39757                                               None  None  None  None   \n",
      "39758                                               None  None  None  None   \n",
      "39759          but because I'm lazier than you. #sarcasm  None  None  None   \n",
      "39760                                               None  None  None  None   \n",
      "39761   and not even be able to look at the other gen...  None  None  None   \n",
      "39762                                               None  None  None  None   \n",
      "39763                                               None  None  None  None   \n",
      "39764                                               None  None  None  None   \n",
      "39765                                               None  None  None  None   \n",
      "39766                                               None  None  None  None   \n",
      "39767                                               None  None  None  None   \n",
      "39768                                               None  None  None  None   \n",
      "39769                                               None  None  None  None   \n",
      "39770                                               None  None  None  None   \n",
      "39771                                               None  None  None  None   \n",
      "39772                                               None  None  None  None   \n",
      "39773                                               None  None  None  None   \n",
      "39774                                               None  None  None  None   \n",
      "39775                                               None  None  None  None   \n",
      "39776                                               None  None  None  None   \n",
      "39777                                               None  None  None  None   \n",
      "39778                                               None  None  None  None   \n",
      "39779                                               None  None  None  None   \n",
      "\n",
      "          6     7     8  \n",
      "0      None  None  None  \n",
      "1      None  None  None  \n",
      "2      None  None  None  \n",
      "3      None  None  None  \n",
      "4      None  None  None  \n",
      "5      None  None  None  \n",
      "6      None  None  None  \n",
      "7      None  None  None  \n",
      "8      None  None  None  \n",
      "9      None  None  None  \n",
      "10     None  None  None  \n",
      "11     None  None  None  \n",
      "12     None  None  None  \n",
      "13     None  None  None  \n",
      "14     None  None  None  \n",
      "15     None  None  None  \n",
      "16     None  None  None  \n",
      "17     None  None  None  \n",
      "18     None  None  None  \n",
      "19     None  None  None  \n",
      "20     None  None  None  \n",
      "21     None  None  None  \n",
      "22     None  None  None  \n",
      "23     None  None  None  \n",
      "24     None  None  None  \n",
      "25     None  None  None  \n",
      "26     None  None  None  \n",
      "27     None  None  None  \n",
      "28     None  None  None  \n",
      "29     None  None  None  \n",
      "...     ...   ...   ...  \n",
      "39750  None  None  None  \n",
      "39751  None  None  None  \n",
      "39752  None  None  None  \n",
      "39753  None  None  None  \n",
      "39754  None  None  None  \n",
      "39755  None  None  None  \n",
      "39756  None  None  None  \n",
      "39757  None  None  None  \n",
      "39758  None  None  None  \n",
      "39759  None  None  None  \n",
      "39760  None  None  None  \n",
      "39761  None  None  None  \n",
      "39762  None  None  None  \n",
      "39763  None  None  None  \n",
      "39764  None  None  None  \n",
      "39765  None  None  None  \n",
      "39766  None  None  None  \n",
      "39767  None  None  None  \n",
      "39768  None  None  None  \n",
      "39769  None  None  None  \n",
      "39770  None  None  None  \n",
      "39771  None  None  None  \n",
      "39772  None  None  None  \n",
      "39773  None  None  None  \n",
      "39774  None  None  None  \n",
      "39775  None  None  None  \n",
      "39776  None  None  None  \n",
      "39777  None  None  None  \n",
      "39778  None  None  None  \n",
      "39779  None  None  None  \n",
      "\n",
      "[39780 rows x 9 columns]>\n"
     ]
    }
   ],
   "source": [
    "X, y = [], []\n",
    "punct = string.punctuation\n",
    "stemmer = PorterStemmer() \n",
    "\n",
    "# Lowercase_sentences\n",
    "# for d in data[1:]: \n",
    "#     label = d[0]\n",
    "#     text = d[1]\n",
    "#     text = text.lower()\n",
    "    \n",
    "    #Remove punctuation\n",
    "#     text = [c for c in text if not (c in punct)]    \n",
    "#     text = ' '.join(text)    \n",
    "    \n",
    "    #Stemming and removing stop words gave worse accuracy\n",
    "    \n",
    "    #Remove stop words\n",
    "    #stops = set(stopwords.words(\"english\"))\n",
    "    #text = [w for w in text if not w in stops and len(w) >= 3]\n",
    " \n",
    "    #Stem the words\n",
    "    #text = [stemmer.stem(w) for w in text]\n",
    "#     X.append(text)\n",
    "#     Y.append(int(label))\n",
    "#The model automatically splits train and validation\n",
    "# X_train = X[:700000]\n",
    "# Y_Train = Y[:700000]\n",
    "# X_Test = X[700000:900000]\n",
    "# Y_Test = Y[700000:900000]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2,random_state=42)\n",
    "# print(len(X_train),len(X_test),len(y_train),len(y_test))\n",
    "print(df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 20000\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences = tokenizer.texts_to_sequences(X_train)\n",
    "dat = pad_sequences(sequences, maxlen=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(20000, 100, input_length=50))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 0 samples, validate on 0 samples\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ProgbarLogger' object has no attribute 'log_values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-806ed7d2df61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_Train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'LSTM_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/env36/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda2/envs/env36/lib/python3.5/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    215\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                             \u001b[0mepoch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/env36/lib/python3.5/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/env36/lib/python3.5/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ProgbarLogger' object has no attribute 'log_values'"
     ]
    }
   ],
   "source": [
    "model.fit(dat, np.array(Y_Train), validation_split=0.4, epochs=3)\n",
    "model.save('LSTM_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the model\n",
    "sent = input(\"Give me a sentence:\")\n",
    "sent = sent.lower()\n",
    "\n",
    "model.load_weights('Reddit_LSTM.h5')\n",
    "out = model.predict(sent)\n",
    "if sum(out)/len(out):\n",
    "    print(\"Your sentence is sarcastic\")\n",
    "else:\n",
    "    print(\"Your sentence is not sarcastic\")\n",
    "predict = model.predict(pad_sequences(tokenizer.texts_to_sequences(X_Test),maxlen=50))\n",
    "correct = 0\n",
    "output = []\n",
    "for i in predict:\n",
    "    if i<0.50:\n",
    "        output.append(0)\n",
    "    else:\n",
    "        output.append(1)\n",
    "        \n",
    "print(output[:10])\n",
    "print(Y_Test[:10])\n",
    "for i in range(0,len(Y_Test)):\n",
    "    if output[i] == Y_Test[i]:\n",
    "        correct+=1\n",
    "        \n",
    "print(\"Test Accuracy: \"+ str(correct/len(Y_Test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
