{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from sys import platform\n",
    "\n",
    "sys.path.append('../')\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models.wrappers import FastText\n",
    "import numpy as np\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import itertools\n",
    "import shutil\n",
    "import hashlib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_unicode_mapping(path):\n",
    "    emoji_dict = defaultdict()\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            tokens = line.strip().split('\\t')\n",
    "            emoji_dict[tokens[0]] = tokens[1]\n",
    "    return emoji_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word2vec(path=None):\n",
    "    word2vecmodel = KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "    return word2vecmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fasttext(path=None):\n",
    "    word2vecmodel = FastText.load_fasttext_format(path)\n",
    "    return word2vecmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InitializeWords(word_file_path):\n",
    "    word_dictionary = defaultdict()\n",
    "    \n",
    "    with open(word_file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            tokens = line.lower().strip().split('\\t')\n",
    "            word_dictionary[tokens[0]] = int(tokens[1])\n",
    "            \n",
    "    for alphabet in \"bcdefghijklmnopqrstuvwxyz\":\n",
    "        if (alphabet in word_dictionary):\n",
    "                word_dictionary.__delitem__(alphabet)\n",
    "                \n",
    "    for word in ['ann', 'assis'\n",
    "                 'bz',\n",
    "                 'ch', 'cre', 'ct',\n",
    "                 'di',\n",
    "                 'ed', 'ee',\n",
    "                 'ic',\n",
    "                 'le',\n",
    "                 'ng', 'ns',\n",
    "                 'pr', 'picon',\n",
    "                 'th', 'tle', 'tl', 'tr',\n",
    "                 'um',\n",
    "                 've',\n",
    "                 'yi']:\n",
    "        if (word in word_dictionary):\n",
    "            word_dictionary.__delitem__(word)\n",
    "                \n",
    "    return word_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_word(word):\n",
    "    temp = word\n",
    "    while True:\n",
    "        w = re.sub(r\"([a-zA-Z])\\1\\1\", r\"\\1\\1\", temp)\n",
    "        if (w == temp):\n",
    "            break\n",
    "        else:\n",
    "            temp = w\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split_word(split_word_file_path):\n",
    "    split_word_dictionary = defaultdict()\n",
    "    with open(split_word_file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            tokens = line.lower().strip().split('\\t')\n",
    "            if (len(tokens) >= 2):\n",
    "                split_word_dictionary[tokens[0]] = tokens[1]\n",
    "    \n",
    "    print('split entry found:', len(split_word_dictionary.keys()))\n",
    "    return split_word_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_hashtags(term, wordlist, split_word_list, dump_file=''):\n",
    "    if (len(term.strip()) == 1):\n",
    "        return ['']\n",
    "    \n",
    "    if (split_word_list != None and term.lower() in split_word_list):\n",
    "        return split_word_list.get(term.lower()).split(' ')\n",
    "    else:\n",
    "        print(term)\n",
    "        \n",
    "    if (term.startswith('#')):\n",
    "        term = term[1:]\n",
    "    \n",
    "    if (wordlist != None and term.lower() in wordlist):\n",
    "        return [term.lower()]\n",
    "    \n",
    "    words = []\n",
    "    penalty = -69971\n",
    "    max_coverage = penalty\n",
    "    \n",
    "    split_words_count = 6\n",
    "    \n",
    "    term = re.sub(r'([0-9]+)', r' \\1', term)\n",
    "    term = re.sub(r'(1st|2nd|3rd|4th|5th|6th|7th|8th|9th|0th)', r'\\1', term)\n",
    "    term = re.sub(r'([A-Z][^A-Z ]+)', r' \\1', term.strip())\n",
    "    term = re.sub(r'([A=Z]{2,})+', r' \\1', term)\n",
    "    words = term.strip().split(' ')\n",
    "    \n",
    "    n_splits = 0\n",
    "    \n",
    "    if (len(words) < 3):\n",
    "        chars = [c for c in term.lower()]\n",
    "        \n",
    "        found_all_words = False\n",
    "        \n",
    "        while (n_splits < split_words_count and not found_all_words):\n",
    "            for idx in itertools.combinations(range(0, len(chars)), n_splits):\n",
    "                output = np.split(chars, idx)\n",
    "                line = [''.join(o) for o in output]\n",
    "                \n",
    "                score = (1. / len(line)) *sum([wordlist.get( word.strip()) \\\n",
    "                                               if word.strip() in wordlist \\\n",
    "                                               else 0. if word.strip().isnumeric() \\\n",
    "                                               else penalty for word in line])\n",
    "                \n",
    "                if (score > max_coverage):\n",
    "                    words = line\n",
    "                    max_coverage = score\n",
    "                    \n",
    "                    line_is_valid_word = [word.strip() in wordlist \\\n",
    "                                         if not word.isnumeric()\\\n",
    "                                         else True for word in line]\n",
    "                    \n",
    "                    if (all(line_is_valid_word)):\n",
    "                        found_all_words = True\n",
    "            n_splits += 1\n",
    "    \n",
    "    with open(dump_file, 'a', encoding='utf-8') as f:\n",
    "        if (term != '' and len(words) > 0):\n",
    "            f.write('#'+str(term).strip() + '\\t' + ' '.join(words) + '\\t' + str(n_splits) + '\\n')\n",
    "        \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_abbreviation(path='../data/twitter/abbreviations.txt'):\n",
    "    abbreviation_dict = defaultdict()\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            token = line.lower().strip().split('\\t')\n",
    "            abbreviation_dict[token[0]] = token[1]\n",
    "        return abbreviation_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_text(text, word_list, split_word_list, emoji_dict, abbreviation_dict,\n",
    "                normalize_text = False, split_hashtag = False,\n",
    "                ignore_profiles = False, replace_emoji= True):\n",
    "    filtered_text = []\n",
    "    \n",
    "    filter_list = ['/', '-', '=', '+', '…', '\\\\', '(', ')', '&', ':']\n",
    "    \n",
    "    for t in text:\n",
    "        word_tokens = None\n",
    "        \n",
    "        if (ignore_profiles and str(t).startswith(\"@\")):\n",
    "            continue\n",
    "            \n",
    "        if (str(t).startswith('http')):\n",
    "            continue\n",
    "            \n",
    "        if (replace_emoji):\n",
    "            if (t in emoji_dict):\n",
    "                t = emoji_dict.get(t).split('_')\n",
    "                filtered_text.extend(t)\n",
    "                continue\n",
    "        if (split_hashtag and str(t).startswith(\"#\")):\n",
    "            splits = split_hashtags(t, word_list, split_word_list, \n",
    "                                    dump_file='../data/twitter/hastash_split_dump.txt')\n",
    "            if (splits != None):\n",
    "                filtered_text.extend([s for s in splits if (not filtered_text.__contains__(s))])\n",
    "                continue\n",
    "        \n",
    "        if (normalize_text):\n",
    "            t = normalize_word(t)\n",
    "            \n",
    "        if (t in abbreviation_dict):\n",
    "            tokens = abbreviation_dict.get(t).split(' ')\n",
    "            filtered_text.extend(tokens)\n",
    "            continue\n",
    "        \n",
    "        filtered_text.append(t)\n",
    "    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsedata(lines, word_list, split_word_list, emoji_dict, abbreviation_dict,\n",
    "              normalize_text = False, split_hashtag = False, ignore_profiles = False,\n",
    "              lower_case = False, replace_emoji = True, n_grams = None, at_character = False):\n",
    "    data = []\n",
    "    for i, line in enumerate(lines):\n",
    "        if (i % 100 == 0):\n",
    "            print(str(i) + '...', end='', flush=True)\n",
    "        \n",
    "        try:\n",
    "            if (lower_case):\n",
    "                line = line.lower()\n",
    "            \n",
    "            token = line.split('\\t')\n",
    "            \n",
    "            id = token[0]\n",
    "            \n",
    "            label = int(token[1].strip())\n",
    "            \n",
    "            target_text = TweetTokenizer().tokenize(token[2].strip())\n",
    "            if (at_character):\n",
    "                target_text = [c for c in token[2].strip()]\n",
    "                \n",
    "            if (n_grams != None):\n",
    "                n_grams_list = list(create_ngram_set(target_text, ngram_values=n_grams))\n",
    "                target_text.extend(['_'.join(n) for n in n_grams_list])\n",
    "                \n",
    "            target_text = filter_text(target_text, word_list, split_word_list, emoji_dict,\n",
    "                                      abbreviation_dict, normalize_text, split_hashtag,\n",
    "                                      ignore_profiles, replace_emoji=replace_emoji)\n",
    "            \n",
    "            dimensions = []\n",
    "            if (len(token) > 3 and token[3].strip() != 'NA'):\n",
    "                dimensions = [dimension.split('@@')[1] for dimension in token[3].strip().split('|')]\n",
    "                \n",
    "            context = []\n",
    "            if (len(token) > 4):\n",
    "                if (token[4] != 'NA'):\n",
    "                    context = TweetTokenizer().tokenize(token[4].strip())\n",
    "                    context = filter_text(context, word_list, split_word_list, emoji_dict,\n",
    "                                         abbreviation_dict, normalize_text, split_hashtag,\n",
    "                                          ignore_profiles, replace_emoji = replace_emoji)\n",
    "                    \n",
    "            author = 'NA'\n",
    "            if (len(token) > 5):\n",
    "                author = token[5]\n",
    "            \n",
    "            if (len(target_text) != 0):\n",
    "                data.append((id, label, target_text, dimensions, context, author))\n",
    "        except:\n",
    "            raise\n",
    "    print('')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loaddata(filename, word_file_path, split_word_path, emoji_file_path, normalize_text=False,\n",
    "             split_hashtag=False, ignore_profiles=False, lower_case=True, replace_emoji=True,\n",
    "             n_grams=None, at_character=False):\n",
    "    word_list = None\n",
    "    emoji_dict = None\n",
    "    \n",
    "    split_word_list = load_split_word(split_word_path)\n",
    "    \n",
    "    if (split_hashtag):\n",
    "        word_list = InitializeWords(word_file_path)\n",
    "        \n",
    "    if (replace_emoji):\n",
    "        emoji_dict = load_unicode_mapping(emoji_file_path)\n",
    "        \n",
    "    abbreviation_dict = load_abbreviation()\n",
    "    \n",
    "    lines = open(filename, 'r', encoding='utf-8').readlines()\n",
    "    \n",
    "    data = parsedata(lines, word_list, split_word_list, emoji_dict, abbreviation_dict, \n",
    "                     normalize_text=normalize_text, split_hashtag=split_hashtag,\n",
    "                     ignore_profiles=ignore_profiles, lower_case=lower_case, \n",
    "                     replace_emoji=replace_emoji, n_grams=n_grams, at_character=at_character)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(data, without_dimension=True, ignore_context=False, min_freq=0):\n",
    "    vocab = defaultdict(int)\n",
    "    vocab_freq = defaultdict(int)\n",
    "    \n",
    "    total_words = 1\n",
    "    if ( not without_dimension):\n",
    "        for i in range(1, 101):\n",
    "            vocab_freq[str(i)] = 0\n",
    "    for sentence_no, token in enumerate(data):\n",
    "        for word in token[2]:\n",
    "            if (word not in vocab_freq):\n",
    "                vocab_freq[word] = 0\n",
    "            vocab_freq[word] = vocab_freq.get(word) + 1\n",
    "        \n",
    "        if (not without_dimension):\n",
    "            for word in token[3]:\n",
    "                vocab_freq[word] = vocab_freq.get(word) + 1\n",
    "        \n",
    "        if (ignore_context == False):\n",
    "            for word in token[4]:\n",
    "                if (not word in vocab):\n",
    "                    vocab_freq[word] = 0\n",
    "                vocab_freq[word] = vocab_freq.get(word) + 1\n",
    "    \n",
    "    for k, v in vocab_freq.items():\n",
    "        if (v >= min_freq):\n",
    "            vocab[k] = total_words\n",
    "            total_words = total_words + 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_reverse_vocab(vocab):\n",
    "    rev_vocab = defaultdict(str)\n",
    "    for k, v in vocab.items():\n",
    "        rev_vocab[v] = k\n",
    "    return rev_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_word_dimension(data, vocab, drop_dimension_index = None):\n",
    "    X, Y, D, C, A = [], [], [], [], []\n",
    "    \n",
    "    known_words_set = set()\n",
    "    unknown_words_set = set()\n",
    "    \n",
    "    tokens = 0\n",
    "    token_coverage = 0\n",
    "    \n",
    "    for id, label, line, dimensions, context, author in data:\n",
    "        vec = []\n",
    "        context_vec = []\n",
    "        if (len(dimensions) != 0):\n",
    "            dvec = [vocab.get(d) for d in dimensions]\n",
    "        else:\n",
    "            dvec = [vocab.get('unk')] * 11\n",
    "        \n",
    "        if drop_dimension_index != None:\n",
    "            dvec.pop(drop_dimensions_index)\n",
    "            \n",
    "        for words in line:\n",
    "            tokens = tokens + 1\n",
    "            if (words in vocab):\n",
    "                vec.append(vocab[words])\n",
    "                token_coverage = token_coverage + 1\n",
    "                known_words_set.add(words)\n",
    "            else:\n",
    "                vec.append(vocab['unk'])\n",
    "                unknown_words_set.add(words)\n",
    "        else:\n",
    "            context_vec = [vocab['unk']]\n",
    "            \n",
    "        X.append(vec)\n",
    "        Y.append(label)\n",
    "        D.append(dvec)\n",
    "        C.append(context_vec)\n",
    "        A.append(author)\n",
    "        \n",
    "    print('Token coverage:', token_coverage / float(tokens))\n",
    "    print('Word coverage:', len(known_words_set) / float(len(vocab.keys())))\n",
    "    return np.asarray(X), np.asarray(Y), np.asarray(D), np.asarray(C), np.asarray(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence_1d(sequences, maxlen=None, dtype='float32',\n",
    "                    padding='pre', truncating='pre', value=0.):\n",
    "    X = [vectors for vectors in sequences]\n",
    "    \n",
    "    nb_samples = len(X)\n",
    "    \n",
    "    x = (np.zeros((nb_samples, maxlen)) * value).astype(dtype)\n",
    "    \n",
    "    for idx, s in enumerate(X):\n",
    "        if truncating == 'pre':\n",
    "            trunc = s[-maxlen:]\n",
    "        elif truncation == 'post':\n",
    "            trunc = s[:maxlen]\n",
    "        else:\n",
    "            raise ValueError(\"padding type '%s' not understood\" % padding)\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_vocab(filepath, vocab):\n",
    "    with open(filepath, 'w', encoding='utf-8') as fw:\n",
    "        for key, value in vocab.items():\n",
    "            fw.write(str(key) + '\\t' + str(value) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fasttext_weight(vocab, n=300, path=None):\n",
    "    fasttextmodel = load_fasttext(path=path)\n",
    "    emb_weights = np.zeros((len(vocab.keys()) + 1, n))\n",
    "    for k, v in vocab.items():\n",
    "        if (fasttext.__contains__(k)):\n",
    "            emb_weights[v, :] = fasttext[k][:n]\n",
    "    \n",
    "    return emb_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2vec_weight(vocab, n=300, path=None):\n",
    "    word2vecmodel = load_word2vec(path=path)\n",
    "    emb_weights = np.zeros((len(vocab.keys()) + 1, n))\n",
    "    for k, v in vocab.items():\n",
    "        if (word2vecmodel.__contains__(k)):\n",
    "            emb_weights[v, :] = word2vecmodel[k][:n]\n",
    "\n",
    "    return emb_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngram_set(input_list, ngram_value=2):\n",
    "    \"\"\"\n",
    "    Extract a set of n-grams from a list of integers.\n",
    "    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)\n",
    "    {(4, 9), (4, 1), (1, 4), (9, 4)}\n",
    "    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)\n",
    "    [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]\n",
    "    \"\"\"\n",
    "    return set(zip(*[input_list[i:] for i in range(ngram_value)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepend_line(infile, outfile, line):\n",
    "    with open(infile, 'r', encoding='utf-8') as old:\n",
    "        with open(outfile, 'w', encoding='utf-8') as new:\n",
    "            new.write(str(line) + \"\\n\")\n",
    "            shutil.copyfileobj(old, new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepend_slow(infile, outfile, line):\n",
    "    with open(infile, 'r', encoding='utf-8') as fin:\n",
    "        with open(outfile, 'w', encoding='utf-8') as fout:\n",
    "            fout.write(line + \"\\n\")\n",
    "            for line in fin:\n",
    "                fout.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checksum(filename):\n",
    "    BLOCKSIZE = 65536\n",
    "    hasher = hashlib.md5()\n",
    "    with open(filename, 'rb') as afile:\n",
    "        buf = afile.read(BLOCKSIZE)\n",
    "        while len(buf) > 0:\n",
    "            hasher.update(buf)\n",
    "            buf = afile.read(BLOCKSIZE)\n",
    "    return hasher.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_num_lines = {\"glove.840B.300d.txt\": 2196017, \"glove.42B.300d.txt\": 1917494}\n",
    "\n",
    "pretrain_checksum = {\n",
    "    \"glove.6B.300d.txt\": \"b78f53fb56ec1ce9edc367d2e6186ba4\",\n",
    "    \"glove.twitter.27B.50d.txt\": \"6e8369db39aa3ea5f7cf06c1f3745b06\",\n",
    "    \"glove.42B.300d.txt\": \"01fcdb413b93691a7a26180525a12d6e\",\n",
    "    \"glove.6B.50d.txt\": \"0fac3659c38a4c0e9432fe603de60b12\",\n",
    "    \"glove.6B.100d.txt\": \"dd7f3ad906768166883176d69cc028de\",\n",
    "    \"glove.twitter.27B.25d.txt\": \"f38598c6654cba5e6d0cef9bb833bdb1\",\n",
    "    \"glove.6B.200d.txt\": \"49fa83e4a287c42c6921f296a458eb80\",\n",
    "    \"glove.840B.300d.txt\": \"eec7d467bccfa914726b51aac484d43a\",\n",
    "    \"glove.twitter.27B.100d.txt\": \"ccbdddec6b9610196dd2e187635fee63\",\n",
    "    \"glove.twitter.27B.200d.txt\": \"e44cdc3e10806b5137055eeb08850569\",\n",
    "}\n",
    "\n",
    "def check_num_lines_in_glove(filename, check_checksum=False):\n",
    "    if check_checksum:\n",
    "        assert checksum(filename) == pretrain_checksum[filename]\n",
    "    if filename.startswith('glove.6B.'):\n",
    "        return 400000\n",
    "    elif filename.startswith('glove.twitter.27B.'):\n",
    "        return 1193514\n",
    "    else:\n",
    "        return pretrain_num_lines[filename]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_word2vec(filename):\n",
    "    # Input: GloVe Model File\n",
    "    # More models can be downloaded from http://nlp.stanford.edu/projects/glove/\n",
    "    # print(filename[filename.rfind('/')+1:])\n",
    "\n",
    "    glove_file = filename[filename.rfind('/')+1:]\n",
    "    _, _,tokens, dimensions, _ = glove_file.split('.')\n",
    "    num_lines = check_num_lines_in_glove(glove_file)\n",
    "    dims = int(dimensions[:-1])\n",
    "\n",
    "    # Output: Gensim Model text format.\n",
    "    gensim_file = '/data/twitter/glove/glove_model.txt'\n",
    "    gensim_first_line = \"{} {}\".format(num_lines, dims)\n",
    "\n",
    "    # Prepends the line.\n",
    "    if platform == \"linux\" or platform == \"linux2\":\n",
    "        prepend_line(filename, gensim_file, gensim_first_line)\n",
    "    else:\n",
    "        prepend_slow(filename, gensim_file, gensim_first_line)\n",
    "\n",
    "    # Demo: Loads the newly created glove_model.txt into gensim API.\n",
    "    model = gensim.models.Word2Vec.load_word2vec_format(gensim_file, binary=False)  # GloVe Model\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove_model(vocab, n=200):\n",
    "    word2vecmodel = load_glove_word2vec('../data/twitter/glove/glove.twitter.27B/glove.twitter.27B.200d.txt')\n",
    "\n",
    "    emb_weights = np.zeros((len(vocab.keys()) + 1, n))\n",
    "    for k, v in vocab.items():\n",
    "        if (word2vecmodel.__contains__(k)):\n",
    "            emb_weights[v, :] = word2vecmodel[k][:n]\n",
    "\n",
    "    return emb_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "    basepath = os.path.dirname(os.getcwd())\n",
    "    tw_train_file = basepath + '/data/twitter/train/Train_v1.txt'\n",
    "    tw_validation_file = basepath + '/data/twitter/Dev_v1.txt'\n",
    "    tw_test_file = basepath + '/data/twitter/test/Test_v1.txt'\n",
    "    tw_word_file_path = basepath + '/data/twitter/word_list_freq.txt'\n",
    "    tw_split_word_path = basepath + '/data/twitter/word_split.txt'\n",
    "    tw_emoji_file_path = basepath + '/data/twitter/emoji_unicode_names_final.txt'\n",
    "    comb_train_file = basepath + '/data/combinded/train/Train_v1.txt'\n",
    "    comb_validation_file = basepath + '/data/combinded/Dev_v1.txt'\n",
    "    comb_test_file = basepath + '/data/combinded/test/Test_v1.txt'\n",
    "    comb_word_file_path = basepath + '/data/combinded/word_list_freq.txt'\n",
    "    comb_split_word_path = basepath + '/data/combinded/word_split.txt'\n",
    "    comb_emoji_file_path = basepath + '/data/combinded/emoji_unicode_names_final.txt'\n",
    "\n",
    "    tw_lstm_output_file = basepath + '/models/twitter/LSTM/TestResults.txt'\n",
    "    tw_lstm_model_file = basepath + '/models/twitter/LSTM/weights/'\n",
    "    tw_lstm_vocab_file_path = basepath + '/models/twitter/LSTM/vocab_list.txt'\n",
    "    tw_cnn_output_file = basepath + '/models/twitter/CNN/TestResults.txt'\n",
    "    tw_cnn_model_file = basepath + '/models/twitter/CNN/weights/'\n",
    "    tw_cnn_vocab_file_path = basepath + '/models/twitter/CNN/vocab_list.txt'\n",
    "    tw_dnn_output_file = basepath + '/models/twitter/DNN/TestResults.txt'\n",
    "    tw_dnn_model_file = basepath + '/models/twitter/DNN/weights/'\n",
    "    tw_dnn_vocab_file_path = basepath + '/models/twitter/DNN/vocab_list.txt'\n",
    "    comb_lstm_output_file = basepath + '/models/combinded/LSTM/TestResults.txt'\n",
    "    comb_lstm_model_file = basepath + '/models/combinded/LSTM/weights/'\n",
    "    comb_lstm_vocab_file_path = basepath + '/models/combinded/LSTM/vocab_list.txt'\n",
    "    comb_cnn_output_file = basepath + '/models/combinded/CNN/TestResults.txt'\n",
    "    comb_cnn_model_file = basepath + '/models/combinded/CNN/weights/'\n",
    "    comb_cnn_vocab_file_path = basepath + '/models/combinded/CNN/vocab_list.txt'\n",
    "    comb_dnn_output_file = basepath + '/models/combinded/DNN/TestResults.txt'\n",
    "    comb_dnn_model_file = basepath + '/models/combinded/DNN/weights/'\n",
    "    comb_dnn_vocab_file_path = basepath + '/models/combinded/DNN/vocab_list.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337)\n",
    "from sklearn import metrics\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers.core import Dropout, Dense, Activation, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from collections import defaultdict\n",
    "import collections\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sarcasm_model():\n",
    "    _train_file = None\n",
    "    _test_file = None\n",
    "    _tweet_file = None\n",
    "    _output_file = None\n",
    "    _model_file_path = None\n",
    "    _word_file_path = None\n",
    "    _split_word_file_path = None\n",
    "    _emoji_file_path = None\n",
    "    _vocab_file_path = None\n",
    "    _input_weight_file_path = None\n",
    "    _vocab = None\n",
    "    _line_maxlen = None\n",
    "    def __init__(self):\n",
    "        self._line_maxlen = 30\n",
    "        \n",
    "    def _build_CNN_LSTM_DNN_network(self, vocab_size, maxlen, embedding_dimension=256,\n",
    "                                    hidden_units=256, trainable=False):\n",
    "        print('Build model...')\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Embedding(vocab_size, embedding_dimension, input_length=maxlen,\n",
    "                           embeddings_initializer='glorot_normal'))\n",
    "        \n",
    "        model.add(Convolution1D(hidden_units, 3, kernel_initializer='he_normal', padding='valid',\n",
    "                               activation='sigmoid',input_shape=(1, maxlen)))\n",
    "        \n",
    "        model.add(Convolution1D(hidden_units, 3, kernel_initializer='he_normal', padding='valid',\n",
    "                               activation='sigmoid',input_shape=(1, maxlen - 2)))\n",
    "        model.add(LSTM(hidden_units, kernel_initializer='he_normal', activation='sigmoid', dropout=0.5,\n",
    "                       return_sequences=True))\n",
    "        \n",
    "        model.add(LSTM(hidden_units, kernel_initializer='he_normal', activation='sigmoid', dropout=0.5))\n",
    "        model.add(Dense(hidden_units, kernel_initializer='he_normal', activation='sigmoid'))\n",
    "        model.add(Dense(2))\n",
    "        model.add(Activation('softmax'))\n",
    "        adam = Adam(lr=0.0001)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "        print('No of parameter:', model.count_params())\n",
    "\n",
    "        print(model.summary())\n",
    "        return model\n",
    "    def _build_LSTM_network(self, vocab_size, maxlen, embedding_dimension=256, hidden_units=256,\n",
    "                            trainable=False):\n",
    "        print('Build model...')\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(vocab_size, embedding_dimension, input_length=maxlen,\n",
    "                           embeddings_initializer='glorot_normal'))\n",
    "        model.add(LSTM(hidden_units, kernel_initializer='he_normal', activation='sigmoid',\n",
    "                       dropout=0.2, return_sequences=True))\n",
    "        model.add(LSTM(2, kernel_initializer='he_normal', activation='sigmoid'))\n",
    "        adam = Adam(lr=0.0001)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "        print('No of parameter:', model.count_params())\n",
    "\n",
    "        print(model.summary())\n",
    "        return model\n",
    "    def _build_CNN_network(self, vocab_size, maxlen, embedding_dimension=256, hidden_units=256,\n",
    "                           trainable=False):\n",
    "        print('Build model...')\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(vocab_size, embedding_dimension, input_length=maxlen,\n",
    "                           embeddings_initializer='glorot_normal'))\n",
    "        \n",
    "        model.add(Convolution1D(hidden_units, 3, kernel_initializer='he_normal', padding='valid',\n",
    "                               activation='sigmoid',input_shape=(1, maxlen)))\n",
    "        model.add(MaxPooling1D(pool_size=3))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Dense(activation = 'softmax', units=2))\n",
    "        adam = Adam(lr=0.0001)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "        print('No of parameter:', model.count_params())\n",
    "\n",
    "        print(model.summary())\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "class train_model(sarcasm_model):\n",
    "    train = None\n",
    "    validation = None\n",
    "    model_type = 'CNN'\n",
    "    print(\"Loading data...\")\n",
    "    \n",
    "    def __init__(self, model_type, train_file, validation_file, word_file_path, split_word_path, emoji_file_path,\n",
    "                 model_file, vocab_file, output_file, input_weight_file_path=None):\n",
    "        sarcasm_model.__init__(self)\n",
    "        self.model_type = model_type\n",
    "        self._train_file = train_file\n",
    "        self._validation_file = validation_file\n",
    "        self._word_file_path = word_file_path\n",
    "        self._split_word_file_path = split_word_path\n",
    "        self._emoji_file_path = emoji_file_path\n",
    "        self._model_file = model_file\n",
    "        self._vocab_file_path = vocab_file\n",
    "        self._output_file = output_file\n",
    "        self._input_weight_file_path = input_weight_file_path\n",
    "        \n",
    "        self.load_train_validation_data()\n",
    "        \n",
    "        print(self._line_maxlen)\n",
    "        \n",
    "        self._vocab = build_vocab(self.train, min_freq=1)\n",
    "        if ('unk' not in self._vocab):\n",
    "            self._vocab['unk'] = len(self._vocab.keys()) + 1\n",
    "        \n",
    "        print(len(self._vocab.keys()) + 1)\n",
    "        print('unk::', self._vocab['unk'])\n",
    "        write_vocab(self._vocab_file_path, self._vocab)\n",
    "        \n",
    "        X, Y, D, C, A = vectorize_word_dimension(self.train, self._vocab)\n",
    "        X = pad_sequence_1d(X, maxlen=self._line_maxlen)\n",
    "        \n",
    "        vX, vY, vD, vC, vA = vectorize_word_dimension(self.validation, self._vocab)\n",
    "        vX = pad_sequence_1d(vX, maxlen=self._line_maxlen)\n",
    "        \n",
    "        dimension_size = 256\n",
    "        \n",
    "        ratio = self.calculate_label_ratio(Y)\n",
    "        ratio = [max(ratio.values()) / value for key, value in ratio.items()]\n",
    "        print('class ratio::', ratio)\n",
    "        \n",
    "        Y, vY = [np_utils.to_categorical(x) for x in (Y, vY)]\n",
    "\n",
    "        print('train_X', X.shape)\n",
    "        print('train_Y', Y.shape)\n",
    "        print('validation_X', vX.shape)\n",
    "        print('validation_Y', vY.shape)\n",
    "        \n",
    "        if model_type == 'CNN':\n",
    "            model = self._build_CNN_network(len(self._vocab.keys()) + 1, self._line_maxlen,\n",
    "                                            embedding_dimension=dimension_size, trainable=True)\n",
    "        elif model_type == 'LSTM':\n",
    "            model = self._build_LSTM_network(len(self._vocab.keys()) + 1, self._line_maxlen,\n",
    "                                             embedding_dimension=dimension_size, trainable=True)\n",
    "        elif model_type == 'DNN':\n",
    "            model = self._build_CNN_LSTM_DNN_network(len(self._vocab.keys()) + 1, self._line_maxlen,\n",
    "                                                     embedding_dimension=dimension_size, \n",
    "                                                     trainable=True)\n",
    "        \n",
    "        open(self._model_file + 'model.json', 'w').write(model.to_json())\n",
    "        save_best = ModelCheckpoint(model_file + 'model.json.hdf5', save_best_only=True)\n",
    "        save_all = ModelCheckpoint(self._model_file + 'weights.{epoch:02d}__.hdf5',\n",
    "                                   save_best_only=False)\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=1)\n",
    "\n",
    "        # training\n",
    "        model.fit(X, Y, batch_size=8, epochs=10, validation_data=(vX, vY), shuffle=True,\n",
    "                  callbacks=[save_best, save_all, early_stopping], class_weight=ratio)\n",
    "        \n",
    "    def load_train_validation_data(self):\n",
    "        self.train = loaddata(self._train_file, self._word_file_path, self._split_word_file_path,\n",
    "                                 self._emoji_file_path, normalize_text=True,\n",
    "                                 split_hashtag=True,\n",
    "                                 ignore_profiles=False)\n",
    "        print('Training data loading finished...')\n",
    "\n",
    "        self.validation = loaddata(self._validation_file, self._word_file_path, self._split_word_file_path,\n",
    "                                      self._emoji_file_path,\n",
    "                                      normalize_text=True,\n",
    "                                      split_hashtag=True,\n",
    "                                      ignore_profiles=False)\n",
    "        print('Validation data loading finished...')\n",
    "\n",
    "        if (self._test_file != None):\n",
    "            self.test = loaddata(self._test_file, self._word_file_path, normalize_text=True,\n",
    "                                    split_hashtag=True,\n",
    "                                    ignore_profiles=True)\n",
    "    def get_maxlen(self):\n",
    "        return max(map(len, (x for _, x in self.train + self.validation)))\n",
    "\n",
    "    def write_vocab(self):\n",
    "        with open(self._vocab_file_path, 'w', encoding='utf-8') as fw:\n",
    "            for key, value in self._vocab.iteritems():\n",
    "                fw.write(str(key) + '\\t' + str(value) + '\\n')\n",
    "\n",
    "    def calculate_label_ratio(self, labels):\n",
    "        return collections.Counter(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class test_model(sarcasm_model):\n",
    "    test = None\n",
    "    model = None\n",
    "\n",
    "    \n",
    "    def __init__(self, model_file, word_file_path, split_word_path, emoji_file_path, vocab_file_path,\n",
    "                output_file, input_weight_file_path=None):\n",
    "        print('Initializing...')\n",
    "        sarcasm_model.__init__(self)\n",
    "        \n",
    "        self._model_file_path = model_file\n",
    "        self._word_file_path = word_file_path\n",
    "        self._split_word_file_path = split_word_path\n",
    "        self._emoji_file_path = emoji_file_path\n",
    "        self._vocab_file_path = vocab_file_path\n",
    "        self._output_file = output_file\n",
    "        self._input_weight_file_path = input_weight_file_path\n",
    "        \n",
    "        print('test_maxlen', self._line_maxlen)\n",
    "    \n",
    "    def load_trained_model(self, model_file='model.json', weight_file='model.json.hdf5'):\n",
    "        start = time.time()\n",
    "        self.__load_model(self._model_file_path + model_file, self._model_file_path + weight_file)\n",
    "        end = time.time()\n",
    "        print('model loading time::', (end - start))\n",
    "\n",
    "    def __load_model(self, model_path, model_weight_path):\n",
    "        self.model = model_from_json(open(model_path, encoding='utf-8').read())\n",
    "        print('model loaded from file...')\n",
    "        self.model.load_weights(model_weight_path)\n",
    "        print('model weights loaded from file...')\n",
    "\n",
    "    def load_vocab(self):\n",
    "        vocab = defaultdict()\n",
    "        with open(self._vocab_file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f.readlines():\n",
    "                key, value = line.split('\\t')\n",
    "                vocab[key] = value\n",
    "        return vocab\n",
    "    \n",
    "    def predict(self, test_file, verbose=False):\n",
    "        try:\n",
    "            start = time.time()\n",
    "            self.test = loaddata(test_file, self._word_file_path, self._split_word_file_path, self._emoji_file_path,\n",
    "                                    normalize_text=True, split_hashtag=True,\n",
    "                                    ignore_profiles=False)\n",
    "            end = time.time()\n",
    "            if (verbose == True):\n",
    "                print('test resource loading time::', (end - start))\n",
    "\n",
    "            self._vocab = self.load_vocab()\n",
    "            print('vocab loaded...')\n",
    "\n",
    "            start = time.time()\n",
    "            tX, tY, tD, tC, tA = vectorize_word_dimension(self.test, self._vocab)\n",
    "            tX = pad_sequence_1d(tX, maxlen=self._line_maxlen)\n",
    "            end = time.time()\n",
    "            if (verbose == True):\n",
    "                print('test resource preparation time::', (end - start))\n",
    "\n",
    "            self.__predict_model(tX, self.test)\n",
    "        except Exception as e:\n",
    "            print('Error:', e)\n",
    "            raise\n",
    "    \n",
    "    def __predict_model(self, tX, test):\n",
    "        y = []\n",
    "        y_pred = []\n",
    "        \n",
    "        prediction_probability = self.model.predict_proba(tX, batch_size=1, verbose=1)\n",
    "        \n",
    "        try:\n",
    "            fd = open(self._output_file + '.analysis', 'w', encoding='utf-8')\n",
    "            for i, (label) in enumerate(prediction_probability):\n",
    "                gold_label = test[i][1]\n",
    "                words = test[i][2]\n",
    "                dimensions = test[i][3]\n",
    "                context = test[i][4]\n",
    "                author = test[i][5]\n",
    "\n",
    "                predicted = np.argmax(prediction_probability[i])\n",
    "\n",
    "                y.append(int(gold_label))\n",
    "                y_pred.append(predicted)\n",
    "\n",
    "                fd.write(str(label[0]) + '\\t' + str(label[1]) + '\\t'\n",
    "                         + str(gold_label) + '\\t'\n",
    "                         + str(predicted) + '\\t'\n",
    "                         + ' '.join(words))\n",
    "\n",
    "                fd.write('\\n')\n",
    "\n",
    "            print()\n",
    "\n",
    "            print('accuracy::', metrics.accuracy_score(y, y_pred))\n",
    "            print('precision::', metrics.precision_score(y, y_pred, average='weighted'))\n",
    "            print('recall::', metrics.recall_score(y, y_pred, average='weighted'))\n",
    "            print('f_score::', metrics.f1_score(y, y_pred, average='weighted'))\n",
    "            print('f_score::', metrics.classification_report(y, y_pred))\n",
    "            fd.close()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            raise\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split entry found: 10942\n",
      "0...100...200...300...400...500...600...700...800...900...1000...1100...1200...1300...1400...1500...1600...1700...1800...1900...2000...2100...2200...2300...2400...2500...2600...2700...2800...2900...3000...3100...3200...3300...3400...3500...3600...3700...3800...3900...4000...4100...4200...4300...#더쇼\n",
      "4400...4500...4600...4700...4800...4900...5000...5100...#갓제븐\n",
      "5200...5300...5400...5500...5600...5700...5800...5900...6000...6100...6200...6300...6400...6500...6600...6700...6800...6900...7000...7100...7200...7300...7400...7500...7600...7700...7800...7900...8000...8100...#1122_기현데이\n",
      "8200...8300...8400...8500...8600...8700...8800...8900...9000...9100...9200...9300...9400...9500...9600...9700...9800...9900...10000...10100...10200...10300...10400...10500...10600...10700...10800...10900...11000...11100...11200...11300...11400...11500...11600...11700...11800...11900...12000...12100...12200...12300...12400...12500...12600...12700...12800...12900...13000...13100...13200...13300...13400...13500...13600...13700...13800...13900...14000...14100...14200...14300...14400...14500...14600...14700...14800...14900...15000...15100...15200...15300...15400...15500...15600...15700...15800...#더쇼\n",
      "15900...16000...16100...16200...16300...16400...16500...16600...16700...16800...16900...17000...17100...17200...17300...17400...17500...17600...17700...17800...17900...18000...18100...18200...18300...18400...18500...18600...18700...18800...18900...19000...19100...19200...19300...19400...19500...19600...19700...19800...#김재중\n",
      "19900...20000...20100...20200...20300...20400...20500...20600...20700...20800...20900...21000...21100...21200...21300...21400...21500...21600...21700...21800...21900...22000...22100...22200...22300...22400...22500...22600...22700...22800...22900...23000...23100...23200...23300...23400...23500...23600...23700...23800...23900...24000...24100...24200...24300...24400...24500...24600...24700...24800...24900...25000...25100...25200...25300...25400...#debate\n",
      "25500...25600...25700...25800...25900...26000...26100...26200...26300...26400...26500...26600...26700...26800...26900...27000...27100...27200...27300...27400...27500...27600...27700...27800...27900...28000...28100...28200...28300...28400...28500...28600...28700...28800...28900...29000...29100...29200...29300...29400...29500...29600...29700...29800...29900...30000...30100...30200...30300...30400...30500...30600...30700...30800...30900...31000...31100...31200...31300...31400...31500...31600...31700...31800...31900...32000...32100...32200...32300...32400...32500...32600...32700...32800...32900...33000...33100...33200...33300...33400...33500...33600...33700...33800...33900...34000...34100...34200...34300...34400...34500...34600...34700...34800...34900...35000...35100...35200...35300...35400...35500...35600...35700...35800...35900...36000...36100...36200...36300...36400...36500...36600...36700...36800...36900...37000...37100...37200...37300...37400...37500...37600...37700...37800...37900...38000...38100...38200...38300...38400...38500...38600...38700...38800...38900...39000...39100...39200...39300...39400...39500...39600...39700...\n",
      "Training data loading finished...\n",
      "split entry found: 10942\n",
      "0...100...200...300...400...500...600...700...800...#bey\n",
      "900...1000...1100...1200...1300...1400...1500...1600...\n",
      "Validation data loading finished...\n",
      "30\n",
      "33892\n",
      "unk:: 33891\n",
      "Token coverage: 1.0\n",
      "Word coverage: 0.9999704936413797\n",
      "Token coverage: 0.9857623432668456\n",
      "Word coverage: 0.13006402879820603\n",
      "class ratio:: [1.0, 1.151665945478148]\n",
      "train_X (39780, 30)\n",
      "train_Y (39780, 2)\n",
      "validation_X (1605, 30)\n",
      "validation_Y (1605, 2)\n",
      "Build model...\n",
      "No of parameter: 9203736\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, 30, 256)           8676352   \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 30, 256)           525312    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 2)                 2072      \n",
      "=================================================================\n",
      "Total params: 9,203,736\n",
      "Trainable params: 9,203,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 39780 samples, validate on 1605 samples\n",
      "Epoch 1/10\n",
      "  152/39780 [..............................] - ETA: 24:22 - loss: 0.6958 - acc: 0.4934"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-82-d1af4509e97a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m twitter_LSTM_tr = train_model('LSTM', tw_train_file, tw_validation_file, tw_word_file_path, \n\u001b[0;32m      2\u001b[0m                               \u001b[0mtw_split_word_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtw_emoji_file_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtw_lstm_model_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m                               tw_lstm_vocab_file_path, tw_lstm_output_file)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-80-933edb8a1512>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model_type, train_file, validation_file, word_file_path, split_word_path, emoji_file_path, model_file, vocab_file, output_file, input_weight_file_path)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;31m# training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         model.fit(X, Y, batch_size=8, epochs=10, validation_data=(vX, vY), shuffle=True,\n\u001b[1;32m---> 71\u001b[1;33m                   callbacks=[save_best, save_all, early_stopping], class_weight=ratio)\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload_train_validation_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mA:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mA:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mA:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mA:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mA:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "twitter_LSTM_tr = train_model('LSTM', tw_train_file, tw_validation_file, tw_word_file_path, \n",
    "                              tw_split_word_path, tw_emoji_file_path, tw_lstm_model_file,\n",
    "                              tw_lstm_vocab_file_path, tw_lstm_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_LSTM_te = test_model(tw_lstm_model_file, tw_word_file_path, tw_split_word_path,\n",
    "                             tw_emoji_file_path, tw_lstm_vocab_file_path, tw_lstm_output_file)\n",
    "twitter_LSTM_te.load_trained_model(weight_file='weights.10__.hdf5')\n",
    "twitter_LSTM_te.predict(tw_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split entry found: 10942\n",
      "0...100...200...300...400...500...600...700...800...900...1000...1100...1200...1300...1400...1500...1600...1700...1800...1900...2000...2100...2200...2300...2400...2500...2600...2700...2800...2900...3000...3100...3200...3300...3400...3500...3600...3700...3800...3900...4000...4100...4200...4300...#더쇼\n",
      "4400...4500...4600...4700...4800...4900...5000...5100...#갓제븐\n",
      "5200...5300...5400...5500...5600...5700...5800...5900...6000...6100...6200...6300...6400...6500...6600...6700...6800...6900...7000...7100...7200...7300...7400...7500...7600...7700...7800...7900...8000...8100...#1122_기현데이\n",
      "8200...8300...8400...8500...8600...8700...8800...8900...9000...9100...9200...9300...9400...9500...9600...9700...9800...9900...10000...10100...10200...10300...10400...10500...10600...10700...10800...10900...11000...11100...11200...11300...11400...11500...11600...11700...11800...11900...12000...12100...12200...12300...12400...12500...12600...12700...12800...12900...13000...13100...13200...13300...13400...13500...13600...13700...13800...13900...14000...14100...14200...14300...14400...14500...14600...14700...14800...14900...15000...15100...15200...15300...15400...15500...15600...15700...15800...#더쇼\n",
      "15900...16000...16100...16200...16300...16400...16500...16600...16700...16800...16900...17000...17100...17200...17300...17400...17500...17600...17700...17800...17900...18000...18100...18200...18300...18400...18500...18600...18700...18800...18900...19000...19100...19200...19300...19400...19500...19600...19700...19800...#김재중\n",
      "19900...20000...20100...20200...20300...20400...20500...20600...20700...20800...20900...21000...21100...21200...21300...21400...21500...21600...21700...21800...21900...22000...22100...22200...22300...22400...22500...22600...22700...22800...22900...23000...23100...23200...23300...23400...23500...23600...23700...23800...23900...24000...24100...24200...24300...24400...24500...24600...24700...24800...24900...25000...25100...25200...25300...25400...#debate\n",
      "25500...25600...25700...25800...25900...26000...26100...26200...26300...26400...26500...26600...26700...26800...26900...27000...27100...27200...27300...27400...27500...27600...27700...27800...27900...28000...28100...28200...28300...28400...28500...28600...28700...28800...28900...29000...29100...29200...29300...29400...29500...29600...29700...29800...29900...30000...30100...30200...30300...30400...30500...30600...30700...30800...30900...31000...31100...31200...31300...31400...31500...31600...31700...31800...31900...32000...32100...32200...32300...32400...32500...32600...32700...32800...32900...33000...33100...33200...33300...33400...33500...33600...33700...33800...33900...34000...34100...34200...34300...34400...34500...34600...34700...34800...34900...35000...35100...35200...35300...35400...35500...35600...35700...35800...35900...36000...36100...36200...36300...36400...36500...36600...36700...36800...36900...37000...37100...37200...37300...37400...37500...37600...37700...37800...37900...38000...38100...38200...38300...38400...38500...38600...38700...38800...38900...39000...39100...39200...39300...39400...39500...39600...39700...\n",
      "Training data loading finished...\n",
      "split entry found: 10942\n",
      "0...100...200...300...400...500...600...700...800...#bey\n",
      "900...1000...1100...1200...1300...1400...1500...1600...\n",
      "Validation data loading finished...\n",
      "30\n",
      "33892\n",
      "unk:: 33891\n",
      "Token coverage: 1.0\n",
      "Word coverage: 0.9999704936413797\n",
      "Token coverage: 0.9857623432668456\n",
      "Word coverage: 0.13006402879820603\n",
      "class ratio:: [1.0, 1.151665945478148]\n",
      "train_X (39780, 30)\n",
      "train_Y (39780, 2)\n",
      "validation_X (1605, 30)\n",
      "validation_Y (1605, 2)\n",
      "Build model...\n",
      "No of parameter: 8873730\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_19 (Embedding)     (None, 30, 256)           8676352   \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 28, 256)           196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 9, 256)            0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 9, 256)            0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 9, 2)              514       \n",
      "=================================================================\n",
      "Total params: 8,873,730\n",
      "Trainable params: 8,873,730\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_6 to have 3 dimensions, but got array with shape (39780, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-96-baa11496c71d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m twitter_CNN_tr = train_model('CNN', tw_train_file, tw_validation_file, tw_word_file_path, \n\u001b[0;32m      2\u001b[0m                               \u001b[0mtw_split_word_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtw_emoji_file_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtw_cnn_model_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m                               tw_cnn_vocab_file_path, tw_cnn_output_file)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-94-933edb8a1512>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model_type, train_file, validation_file, word_file_path, split_word_path, emoji_file_path, model_file, vocab_file, output_file, input_weight_file_path)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;31m# training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         model.fit(X, Y, batch_size=8, epochs=10, validation_data=(vX, vY), shuffle=True,\n\u001b[1;32m---> 71\u001b[1;33m                   callbacks=[save_best, save_all, early_stopping], class_weight=ratio)\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload_train_validation_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mA:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mA:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m                 exception_prefix='target')\n\u001b[0m\u001b[0;32m    790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[1;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mA:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    126\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    129\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected dense_6 to have 3 dimensions, but got array with shape (39780, 2)"
     ]
    }
   ],
   "source": [
    "twitter_CNN_tr = train_model('CNN', tw_train_file, tw_validation_file, tw_word_file_path, \n",
    "                              tw_split_word_path, tw_emoji_file_path, tw_cnn_model_file,\n",
    "                              tw_cnn_vocab_file_path, tw_cnn_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_CNN_te = test_model(tw_cnn_model_file, tw_word_file_path, tw_split_word_path,\n",
    "                             tw_emoji_file_path, tw_cnn_vocab_file_path, tw_cnn_output_file)\n",
    "twitter_CNN_te.load_trained_model(weight_file='weights.05__.hdf5')\n",
    "twitter_CNN_te.predict(tw_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split entry found: 10942\n",
      "0...100...200...300...400...500...600...700...800...900...1000...1100...1200...1300...1400...1500...1600...1700...1800...1900...2000...2100...2200...2300...2400...2500...2600...2700...2800...2900...3000...3100...3200...3300...3400...3500...3600...3700...3800...3900...4000...4100...4200...4300...#더쇼\n",
      "4400...4500...4600...4700...4800...4900...5000...5100...#갓제븐\n",
      "5200...5300...5400...5500...5600...5700...5800...5900...6000...6100...6200...6300...6400...6500...6600...6700...6800...6900...7000...7100...7200...7300...7400...7500...7600...7700...7800...7900...8000...8100...#1122_기현데이\n",
      "8200...8300...8400...8500...8600...8700...8800...8900...9000...9100...9200...9300...9400...9500...9600...9700...9800...9900...10000...10100...10200...10300...10400...10500...10600...10700...10800...10900...11000...11100...11200...11300...11400...11500...11600...11700...11800...11900...12000...12100...12200...12300...12400...12500...12600...12700...12800...12900...13000...13100...13200...13300...13400...13500...13600...13700...13800...13900...14000...14100...14200...14300...14400...14500...14600...14700...14800...14900...15000...15100...15200...15300...15400...15500...15600...15700...15800...#더쇼\n",
      "15900...16000...16100...16200...16300...16400...16500...16600...16700...16800...16900...17000...17100...17200...17300...17400...17500...17600...17700...17800...17900...18000...18100...18200...18300...18400...18500...18600...18700...18800...18900...19000...19100...19200...19300...19400...19500...19600...19700...19800...#김재중\n",
      "19900...20000...20100...20200...20300...20400...20500...20600...20700...20800...20900...21000...21100...21200...21300...21400...21500...21600...21700...21800...21900...22000...22100...22200...22300...22400...22500...22600...22700...22800...22900...23000...23100...23200...23300...23400...23500...23600...23700...23800...23900...24000...24100...24200...24300...24400...24500...24600...24700...24800...24900...25000...25100...25200...25300...25400...#debate\n",
      "25500...25600...25700...25800...25900...26000...26100...26200...26300...26400...26500...26600...26700...26800...26900...27000...27100...27200...27300...27400...27500...27600...27700...27800...27900...28000...28100...28200...28300...28400...28500...28600...28700...28800...28900...29000...29100...29200...29300...29400...29500...29600...29700...29800...29900...30000...30100...30200...30300...30400...30500...30600...30700...30800...30900...31000...31100...31200...31300...31400...31500...31600...31700...31800...31900...32000...32100...32200...32300...32400...32500...32600...32700...32800...32900...33000...33100...33200...33300...33400...33500...33600...33700...33800...33900...34000...34100...34200...34300...34400...34500...34600...34700...34800...34900...35000...35100...35200...35300...35400...35500...35600...35700...35800...35900...36000...36100...36200...36300...36400...36500...36600...36700...36800...36900...37000...37100...37200...37300...37400...37500...37600...37700...37800...37900...38000...38100...38200...38300...38400...38500...38600...38700...38800...38900...39000...39100...39200...39300...39400...39500...39600...39700...\n",
      "Training data loading finished...\n",
      "split entry found: 10942\n",
      "0...100...200...300...400...500...600...700...800...#bey\n",
      "900...1000...1100...1200...1300...1400...1500...1600...\n",
      "Validation data loading finished...\n",
      "30\n",
      "33892\n",
      "unk:: 33891\n",
      "Token coverage: 1.0\n",
      "Word coverage: 0.9999704936413797\n",
      "Token coverage: 0.9857623432668456\n",
      "Word coverage: 0.13006402879820603\n",
      "class ratio:: [1.0, 1.151665945478148]\n",
      "train_X (39780, 30)\n",
      "train_Y (39780, 2)\n",
      "validation_X (1605, 30)\n",
      "validation_Y (1605, 2)\n",
      "Build model...\n",
      "No of parameter: 10187010\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_16 (Embedding)     (None, 30, 256)           8676352   \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 28, 256)           196864    \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 26, 256)           196864    \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 26, 256)           525312    \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 514       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 10,187,010\n",
      "Trainable params: 10,187,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 39780 samples, validate on 1605 samples\n",
      "Epoch 1/10\n",
      "  168/39780 [..............................] - ETA: 26:01 - loss: 0.7126 - acc: 0.4881"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-84-4926ed37f1c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m twitter_DNN_tr = train_model('DNN', tw_train_file, tw_validation_file, tw_word_file_path, \n\u001b[0;32m      2\u001b[0m                               \u001b[0mtw_split_word_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtw_emoji_file_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtw_dnn_model_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m                               tw_dnn_vocab_file_path, tw_dnn_output_file)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-80-933edb8a1512>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model_type, train_file, validation_file, word_file_path, split_word_path, emoji_file_path, model_file, vocab_file, output_file, input_weight_file_path)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;31m# training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         model.fit(X, Y, batch_size=8, epochs=10, validation_data=(vX, vY), shuffle=True,\n\u001b[1;32m---> 71\u001b[1;33m                   callbacks=[save_best, save_all, early_stopping], class_weight=ratio)\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload_train_validation_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mA:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mA:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mA:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mA:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mA:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "twitter_DNN_tr = train_model('DNN', tw_train_file, tw_validation_file, tw_word_file_path, \n",
    "                              tw_split_word_path, tw_emoji_file_path, tw_dnn_model_file,\n",
    "                              tw_dnn_vocab_file_path, tw_dnn_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_DNN_te = test_model(tw_dnn_model_file, tw_word_file_path, tw_split_word_path,\n",
    "                             tw_emoji_file_path, tw_dnn_vocab_file_path, tw_dnn_output_file)\n",
    "twitter_DNN_te.load_trained_model(weight_file='weights.10__.hdf5')\n",
    "twitter_DNN_te.predict(tw_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# tw_tr = pd.read_table(basepath + '/data/twitter/train/Train_v1.txt', encoding='utf-8', header=None)\n",
    "# tw_te = pd.read_table(basepath + '/data/twitter/test/Test_v1.txt', encoding='utf-8', header=None)\n",
    "# re_tr = pd.read_csv(basepath + '/data/combinded/train-balanced-sarcasm.csv', encoding='utf-8')\n",
    "# print(tw_tr.head(),tw_te.head(), re_tr.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# red_dat = re_tr[['label','comment']]\n",
    "# red_dat['Sen_type'] = 'TrainSen'\n",
    "# red_dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = red_dat.columns.tolist()\n",
    "# cols = cols[-1:] + cols[:-1]\n",
    "# red_dat = red_dat[cols]\n",
    "# red_dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twit_dat = tw_tr.merge(tw_te, how='outer')\n",
    "# twit_dat.columns = ['Sen_type', 'label','comment']\n",
    "# twit_dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comb_dat = red_dat.merge(twit_dat, how='outer')\n",
    "# comb_dat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comb_dat = comb_dat.sample(frac=1)\n",
    "# comb_te = comb_dat.iloc[:len(comb_dat)//20, :]\n",
    "# comb_va = comb_dat.iloc[len(comb_dat)//20:2*(len(comb_dat)//20), :]\n",
    "# comb_tr = comb_dat.iloc[2*(len(comb_dat)//20):, :]\n",
    "\n",
    "# print(comb_tr.shape, comb_te.shape, comb_va.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comb_te.to_csv(basepath + '/data/combinded/test/Test_v1.txt', header=False, encoding='utf-8')\n",
    "# comb_va.to_csv(basepath + '/data/combinded/Dev_v1.txt')\n",
    "# comb_te.to_csv(basepath + '/data/combinded/train/Train_v1.txt', header=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
