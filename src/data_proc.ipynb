{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Processing\n",
    "functions:\n",
    "    - load unicode mapping\n",
    "    - load word2vec\n",
    "    - load fast text\n",
    "    - initialize words\n",
    "    - normalize words\n",
    "    - split words\n",
    "    - split hashtags\n",
    "    - load abbreviations\n",
    "    - filter text\n",
    "    - parse data\n",
    "    - load data\n",
    "    - build vocab\n",
    "    - build reverse vocab\n",
    "    - vectorize word dimensions\n",
    "    - pad sequence 1D\n",
    "    - write vocab\n",
    "    - get fasttext weight\n",
    "    - get word2vec weight\n",
    "    - create ngram set\n",
    "    - prepend line\n",
    "    - prepend slow\n",
    "    - checksum\n",
    "    - check num lines in glove\n",
    "    - checksum in glove\n",
    "    - load glove word2vec\n",
    "    - get glove model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from sys import platform\n",
    "\n",
    "sys.path.append('../')\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models.wrappers import FastText\n",
    "import numpy as np\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import itertools\n",
    "import shutil\n",
    "import hashlib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_unicode_mapping(path):\n",
    "    emoji_dict = defaultdict()\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            tokens = line.strip().split('\\t')\n",
    "            emoji_dict[tokens[0]] = token[1]\n",
    "    return emoji_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word2vec(path=None):\n",
    "    word2vecmodel = KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "    return word2vecmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fasttext(path=None):\n",
    "    word2vecmodel = FastText.load_fasttext_format(path)\n",
    "    return word2vecmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InitializeWords(word_file_path):\n",
    "    word_dictionary = defaultdict()\n",
    "    \n",
    "    with open(word_file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            tokens = line.lower().strip().split('\\t')\n",
    "            word_dictionary[tokens[0]] = int(tokens[1])\n",
    "            \n",
    "    for alphabet in \"abcdefghijklmnopqrstuvwxyz\":\n",
    "        if (alphabet in word_dictionary):\n",
    "                word_dictionary.__delitem__(alphabet)\n",
    "                \n",
    "    for word in ['ann', 'assis'\n",
    "                 'bz',\n",
    "                 'ch', 'cre', 'ct',\n",
    "                 'di',\n",
    "                 'ed', 'ee',\n",
    "                 'ic',\n",
    "                 'le',\n",
    "                 'ng', 'ns',\n",
    "                 'pr', 'picon',\n",
    "                 'th', 'tle', 'tl', 'tr',\n",
    "                 'um',\n",
    "                 've',\n",
    "                 'yi']:\n",
    "        if (word in word_dictionary):\n",
    "            word_dictionary.__delitem__(word)\n",
    "                \n",
    "    return word_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_word(word):\n",
    "    temp = word\n",
    "    while True:\n",
    "        w = re.sub(r\"([a-zA-Z])\\1\\1\", r\"\\1\\1\", temp)\n",
    "        if (w == temp):\n",
    "            break\n",
    "        else:\n",
    "            temp = w\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split_word(split_word_file_path):\n",
    "    split_word_dictionary = defaultdict()\n",
    "    with open(split_word_file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            tokens = line.lower().strip().split('\\t')\n",
    "            if (len(tokens) >= 2):\n",
    "                split_word_dictionary[tokens[0]] = tokens[1]\n",
    "    \n",
    "    print('split entry found:', len(split_word_dictionary.keys()))\n",
    "    return split_word_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_hashtags(term, wordlist, split_word_list, dump_file=''):\n",
    "    if (len(term.strip()) == 1):\n",
    "        return ['']\n",
    "    \n",
    "    if (split_word_list != None and term.lower() in split_word_list):\n",
    "        return split_word_list.get(term.lowre()).split(' ')\n",
    "    else:\n",
    "        print(term)\n",
    "        \n",
    "    if (term.startswith('#')):\n",
    "        term = term[1:]\n",
    "    \n",
    "    if (wordlist != None and term.lower() in wordlist):\n",
    "        return [term.lower()]\n",
    "    \n",
    "    words = []\n",
    "    penalty = -69971\n",
    "    max_coverage = penalty\n",
    "    \n",
    "    split_words_count = 6\n",
    "    \n",
    "    term = re.sub(r'([0-9]+)', r' \\1', term)\n",
    "    term = re.sub(r'(1st|2nd|3rd|4th|5th|6th|7th|8th|9th|0th)', r'\\1', term)\n",
    "    term = re.sub(r'([A-Z][^A-Z ]+)', r' \\1', term.strip())\n",
    "    term = re.sub(r'([A=Z]{2,})+', r' \\1', term)\n",
    "    words = term.strip().split(' ')\n",
    "    \n",
    "    n_splits = 0\n",
    "    \n",
    "    if (len(words) < 3):\n",
    "        chars = [c for c in term.lower()]\n",
    "        \n",
    "        found_all_words = False\n",
    "        \n",
    "        while (n_splits < split_words_count and not found_all_words):\n",
    "            for idx in itertools.combinations(range(0, len(chars)), n_splits):\n",
    "                output = np.split(chars, idx)\n",
    "                line = [''.join(o) for o in output]\n",
    "                \n",
    "                score = (1. / len(line)) *sum([wordlist.get( word.strip()) \\\n",
    "                                               if word.strip() in wordlist \\\n",
    "                                               else 0. if word.strip().isnumeric() \\\n",
    "                                               else penalty for word in line])\n",
    "                \n",
    "                if (score > max_coverage):\n",
    "                    words = line\n",
    "                    max_coverage = score\n",
    "                    \n",
    "                    line_is_valid_word = [word.strip() in wordlist \\\n",
    "                                         if not word.isnumeric()\\\n",
    "                                         else True for word in line]\n",
    "                    \n",
    "                    if (all(line_is_valid_word)):\n",
    "                        found_all_words = True\n",
    "            n_splits += 1\n",
    "    \n",
    "    with open(dump_file, 'a') as f:\n",
    "        if (term != '' and len(words) > 0):\n",
    "            f.write('#'+str(term).strip() + '\\t' + ' '.join(words) + '\\t' + str(n_splits) + '\\n')\n",
    "        \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_abbreviation(path='../data/twitter/abbreviations.txt'):\n",
    "    abbreviation_dict = defaultdict()\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            token = line.lower().strip().splti('\\t')\n",
    "            abbreviation_dict[token[0]] = token[1]\n",
    "        return abbreviation_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_text(text, word_list, split_word_list, emoji_dict, abbreviation_dict,\n",
    "                normalize_text = False, split_hashtag = False,\n",
    "                ignore_profiles = False, replace_emoji= True):\n",
    "    filtered_text = []\n",
    "    \n",
    "    filter_list = ['/', '-', '=', '+', 'â€¦', '\\\\', '(', ')', '&', ':']\n",
    "    \n",
    "    for t in text:\n",
    "        word_tokens = None\n",
    "        \n",
    "        if (ignore_profiles and str(t).startswith(\"@\")):\n",
    "            continue\n",
    "            \n",
    "        if (str(t).startswith('http')):\n",
    "            continue\n",
    "            \n",
    "        if (replace_emoji):\n",
    "            if (t in emoji_dict):\n",
    "                t = emoji_dict.get(t).split('_')\n",
    "                filtered_text.extend(t)\n",
    "                continue\n",
    "        if (split_hastag and str(t).startswith(\"#\")):\n",
    "            splits = split_hashtags(t, word_list, split_word_list, \n",
    "                                    dump_file='../data/twitter/hastash_split_dump.txt')\n",
    "            if (splits != None):\n",
    "                filtered_text.extend([s for s in splits if (not filtered_text.__contains__(s))])\n",
    "                continue\n",
    "        \n",
    "        if (normalize_text):\n",
    "            t = normalize_word(t)\n",
    "            \n",
    "        if (t in abbreviation_dict):\n",
    "            tokens = abbreviation_dict.get(t).split(' ')\n",
    "            filtered_text.extend(tokens)\n",
    "            continue\n",
    "        \n",
    "        filtered_text.append(t)\n",
    "    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsedata(lines, word_list, split_word_list, emoji_dict, abbreviation_dict,\n",
    "              normalize_text = False, split_hashtag = False, ignore_profiles = False,\n",
    "              lowercase = False, replace_emoji = True, n_grams = None, at_character = False):\n",
    "    data = []\n",
    "    for i, line in enumerate(lines):\n",
    "        if (i % 100 == 0):\n",
    "            print(str(i) + '...', end='', flush=True)\n",
    "        \n",
    "        try:\n",
    "            if (lowercase):\n",
    "                line = line=lower()\n",
    "            \n",
    "            token = line.split('\\t')\n",
    "            \n",
    "            id = token[0]\n",
    "            \n",
    "            label = int(token[1].strip())\n",
    "            \n",
    "            target_text = TweetTokenizer().tokenize(token[2].strip())\n",
    "            if (at_character):\n",
    "                target_text = [c for c in token[2].strip()]\n",
    "                \n",
    "            if (n_grams != None):\n",
    "                n_grams_list = list(create_ngram_set(target_text, ngram_values=n_grams))\n",
    "                target_text.extend(['_'.join(n) for n in n_grams_list])\n",
    "                \n",
    "            target_text = filter_text(target_text, word_list, split_word_list, emoji_dict,\n",
    "                                      abbreviation_dict, normalize_text, split_hashtag,\n",
    "                                      ignore_profiles, replace_emoji=replace_emoji)\n",
    "            \n",
    "            dimensions = []\n",
    "            if (len(token) > 3 and token[3].strip() != 'NA'):\n",
    "                dimensions = [dimension.split('@@')[1] for dimension in token[3].strip().split('|')]\n",
    "                \n",
    "            context = []\n",
    "            if (len(token) > 4):\n",
    "                if (token[4] != 'NA'):\n",
    "                    context = TweetTokenizer().tokenize(token[4].strip())\n",
    "                    context = filter_text(context, word_list, split_word_list, emoji_dict,\n",
    "                                         abbreviation_dict, normalize_text, split_hashtag,\n",
    "                                          ignore_profiles, replace_emoji = replace_emoji)\n",
    "                    \n",
    "            author = 'NA'\n",
    "            if (len(token) > 5):\n",
    "                auther = token[5]\n",
    "            \n",
    "            if (len(target_text) != 0):\n",
    "                data.append((id, label, target_text, dimensions, context, author))\n",
    "        except:\n",
    "            raise\n",
    "    print('')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loaddata(filename, word_file_path, split_word_path, emoji_file_path, normalize_text=False,\n",
    "             split_hashtag=False, ignore_profiles=False, lower_case=True, replace_emoji=True,\n",
    "             n_grams=None, at_character=False):\n",
    "    word_list = None\n",
    "    emoji_dict = None\n",
    "    \n",
    "    split_word_list = load_split_word(split_word_path)\n",
    "    \n",
    "    if (split_hashtag):\n",
    "        word_list = InitializeWords(word_file_path)\n",
    "        \n",
    "    if (replace_emoji):\n",
    "        emoji_dict = load_unicode_mapping(emoji_file_path)\n",
    "        \n",
    "    abbreviation_dict = load_abbreviation()\n",
    "    \n",
    "    lines = open(filename, 'r').readlines()\n",
    "    \n",
    "    data = parsedata(lines, word_list, split_word_list, emoji_dict, abbreviation_dict, \n",
    "                     normalize_text=normalize_text, split_hashtag=split_hashtag,\n",
    "                     ignore_profiles=ignore_profiles, lowercase=lowercase, \n",
    "                     replace_emoji=replace_emoji, n_grams=n_grams, at_character=at_character)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(data, without_dimension=True, ignore_context=False, min_freq=0):\n",
    "    vocab = defaultdict(int)\n",
    "    vocab_freq = defaultdict(int)\n",
    "    \n",
    "    total_words = 1\n",
    "    if ( not without_dimension):\n",
    "        for i in range(1, 101):\n",
    "            vocab_freq[str(i)] = 0\n",
    "    for sentence_no, token in enumerate(data):\n",
    "        for word in token[2]:\n",
    "            if (word not in voacb_freq):\n",
    "                vocab_freq[word] = 0\n",
    "            vocab_freq[word] = vocab_freq.get(word) + 1\n",
    "        \n",
    "        if (not without_dimension):\n",
    "            for word in token[3]:\n",
    "                vocab_freq[word] = vocab_freq.get(word) + 1\n",
    "        \n",
    "        if (ignore_context == False):\n",
    "            for word in token[4]:\n",
    "                if (not word in vocab):\n",
    "                    vocab_freq[word] = 0\n",
    "                vocab_freq[word] = vocab_freq.get(word) + 1\n",
    "    \n",
    "    for k, v in vocab_freq.items():\n",
    "        if (v >= min_freq):\n",
    "            vocab[k] = total_words\n",
    "            total_words = total_words + 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_reverse_vocab(vocab):\n",
    "    rev_vocab = defaultdict(str)\n",
    "    for k, v in vocab.items():\n",
    "        rev_vocab[v] = k\n",
    "    return rev_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_word_dimension(data, vocab, drop_dimension_index = None):\n",
    "    X, Y, D, C, A = [], [], [], [], []\n",
    "    \n",
    "    known_words_set = set()\n",
    "    unknown_words_set = set()\n",
    "    \n",
    "    tokens = 0\n",
    "    token_coverage = 0\n",
    "    \n",
    "    for id, label, line, dimensions, context, author in data:\n",
    "        vec = []\n",
    "        context_vec = []\n",
    "        if (len(dimensions) != 0):\n",
    "            dvec = [vocab.get(d) for d in dimensions]\n",
    "        else:\n",
    "            dvec = [vocab.get('unk')] * 11\n",
    "        \n",
    "        if drop_dimension_index != None:\n",
    "            dvec.pop(drop_dimensions_index)\n",
    "            \n",
    "        for words in line:\n",
    "            tokens = tokens + 1\n",
    "            if (words in vocab):\n",
    "                vec.append(vocab[words])\n",
    "                token_coverage = token_coverage + 1\n",
    "                known_words_set.add(words)\n",
    "            else:\n",
    "                vec.append(vocab['unk'])\n",
    "                unkown_words_set.add(words)\n",
    "        else:\n",
    "            context_vec = [vocab['unk']]\n",
    "            \n",
    "        X.append(vec)\n",
    "        Y.append(label)\n",
    "        D.append(dvec)\n",
    "        C.append(context_vec)\n",
    "        A.append(author)\n",
    "        \n",
    "    print('Token coverage:', token_coverage / float(tokens))\n",
    "    print('Word coverage:', len(known_words_set) / float(len(vocab.keys())))\n",
    "    return numpy.asarray(X), numpy.asarray(Y), numpy.asarray(D), numpy.asarray(C), numpy.asarray(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence_1d(sequences, maxlen=None, dtype='float32',\n",
    "                    padding='pre', truncating='pre', value=0.):\n",
    "    X = [vectors for vectors in sequences]\n",
    "    \n",
    "    nb_samples = len(X)\n",
    "    \n",
    "    x = (np.zeros((nb_samples, maxlen)) * value).astype(dtype)\n",
    "    \n",
    "    for idx, s in enumerate(X):\n",
    "        if truncating == 'pre':\n",
    "            trunc = s[-maxlen:]\n",
    "        elif truncation == 'post':\n",
    "            trunc = s[:maxlen]\n",
    "        else:\n",
    "            raise ValueError(\"padding type '%s' not understood\" % padding)\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_vocab(filepath, vocab):\n",
    "    with open(filepath, 'w') as fw:\n",
    "        for key, value in vocab.items():\n",
    "            fw.write(str(key) + '\\t' + str(value) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fasttext_weight(vocab, n=300, path=None):\n",
    "    fasttextmodel = load_fasttext(path=path)\n",
    "    emb_weights = np.zeros((len(vocab.keys()) + 1, n))\n",
    "    for k, v in vocab.items():\n",
    "        if (fasttext.__contains__(k)):\n",
    "            emb_weights[v, :] = fasttext[k][:n]\n",
    "    \n",
    "    return emb_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2vec_weight(vocab, n=300, path=None):\n",
    "    word2vecmodel = load_word2vec(path=path)\n",
    "    emb_weights = numpy.zeros((len(vocab.keys()) + 1, n))\n",
    "    for k, v in vocab.items():\n",
    "        if (word2vecmodel.__contains__(k)):\n",
    "            emb_weights[v, :] = word2vecmodel[k][:n]\n",
    "\n",
    "    return emb_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngram_set(input_list, ngram_value=2):\n",
    "    \"\"\"\n",
    "    Extract a set of n-grams from a list of integers.\n",
    "    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)\n",
    "    {(4, 9), (4, 1), (1, 4), (9, 4)}\n",
    "    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)\n",
    "    [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]\n",
    "    \"\"\"\n",
    "    return set(zip(*[input_list[i:] for i in range(ngram_value)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepend_line(infile, outfile, line):\n",
    "    with open(infile, 'r') as old:\n",
    "        with open(outfile, 'w') as new:\n",
    "            new.write(str(line) + \"\\n\")\n",
    "            shutil.copyfileobj(old, new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepend_slow(infile, outfile, line):\n",
    "    with open(infile, 'r') as fin:\n",
    "        with open(outfile, 'w') as fout:\n",
    "            fout.write(line + \"\\n\")\n",
    "            for line in fin:\n",
    "                fout.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checksum(filename):\n",
    "    BLOCKSIZE = 65536\n",
    "    hasher = hashlib.md5()\n",
    "    with open(filename, 'rb') as afile:\n",
    "        buf = afile.read(BLOCKSIZE)\n",
    "        while len(buf) > 0:\n",
    "            hasher.update(buf)\n",
    "            buf = afile.read(BLOCKSIZE)\n",
    "    return hasher.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_num_lines = {\"glove.840B.300d.txt\": 2196017, \"glove.42B.300d.txt\": 1917494}\n",
    "\n",
    "pretrain_checksum = {\n",
    "    \"glove.6B.300d.txt\": \"b78f53fb56ec1ce9edc367d2e6186ba4\",\n",
    "    \"glove.twitter.27B.50d.txt\": \"6e8369db39aa3ea5f7cf06c1f3745b06\",\n",
    "    \"glove.42B.300d.txt\": \"01fcdb413b93691a7a26180525a12d6e\",\n",
    "    \"glove.6B.50d.txt\": \"0fac3659c38a4c0e9432fe603de60b12\",\n",
    "    \"glove.6B.100d.txt\": \"dd7f3ad906768166883176d69cc028de\",\n",
    "    \"glove.twitter.27B.25d.txt\": \"f38598c6654cba5e6d0cef9bb833bdb1\",\n",
    "    \"glove.6B.200d.txt\": \"49fa83e4a287c42c6921f296a458eb80\",\n",
    "    \"glove.840B.300d.txt\": \"eec7d467bccfa914726b51aac484d43a\",\n",
    "    \"glove.twitter.27B.100d.txt\": \"ccbdddec6b9610196dd2e187635fee63\",\n",
    "    \"glove.twitter.27B.200d.txt\": \"e44cdc3e10806b5137055eeb08850569\",\n",
    "}\n",
    "\n",
    "def check_num_lines_in_glove(filename, check_checksum=False):\n",
    "    if check_checksum:\n",
    "        assert checksum(filename) == pretrain_checksum[filename]\n",
    "    if filename.startswith('glove.6B.'):\n",
    "        return 400000\n",
    "    elif filename.startswith('glove.twitter.27B.'):\n",
    "        return 1193514\n",
    "    else:\n",
    "        return pretrain_num_lines[filename]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_word2vec(filename):\n",
    "    # Input: GloVe Model File\n",
    "    # More models can be downloaded from http://nlp.stanford.edu/projects/glove/\n",
    "    # print(filename[filename.rfind('/')+1:])\n",
    "\n",
    "    glove_file = filename[filename.rfind('/')+1:]\n",
    "    _, _,tokens, dimensions, _ = glove_file.split('.')\n",
    "    num_lines = check_num_lines_in_glove(glove_file)\n",
    "    dims = int(dimensions[:-1])\n",
    "\n",
    "    # Output: Gensim Model text format.\n",
    "    gensim_file = '/data/twitter/glove/glove_model.txt'\n",
    "    gensim_first_line = \"{} {}\".format(num_lines, dims)\n",
    "\n",
    "    # Prepends the line.\n",
    "    if platform == \"linux\" or platform == \"linux2\":\n",
    "        prepend_line(filename, gensim_file, gensim_first_line)\n",
    "    else:\n",
    "        prepend_slow(filename, gensim_file, gensim_first_line)\n",
    "\n",
    "    # Demo: Loads the newly created glove_model.txt into gensim API.\n",
    "    model = gensim.models.Word2Vec.load_word2vec_format(gensim_file, binary=False)  # GloVe Model\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove_model(vocab, n=200):\n",
    "    word2vecmodel = load_glove_word2vec('../data/twitter/glove/glove.twitter.27B/glove.twitter.27B.200d.txt')\n",
    "\n",
    "    emb_weights = numpy.zeros((len(vocab.keys()) + 1, n))\n",
    "    for k, v in vocab.items():\n",
    "        if (word2vecmodel.__contains__(k)):\n",
    "            emb_weights[v, :] = word2vecmodel[k][:n]\n",
    "\n",
    "    return emb_weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
