{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Sarcasm Detection using a CNN, LSTM, and DNN"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "cell_type": "markdown",
      "source": "# Preprocessors\nWe combine the data at the bottom of the notebook in \n\nfunctions:\n- load unicode mapping\n- load word2vec\n- load fast text\n- initialize words\n- normalize words\n- split words\n- split hashtags\n- load abbreviations\n- filter text\n- parse data\n- load data\n- build vocab\n- build reverse vocab\n- vectorize word dimensions\n- pad sequence 1D\n- write vocab\n- get fasttext weight\n- get word2vec weight\n- create ngram set\n- prepend line\n- prepend slow\n- checksum\n- check num lines in glove\n- checksum in glove\n- load glove word2vec\n- get glove model"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import os\nimport boto3\nimport re\nfrom sagemaker import get_execution_role\nimport sagemaker.amazon.common as smac\nimport sys\nfrom sys import platform\n\nsys.path.append('../')\nfrom collections import defaultdict\nimport gensim\nfrom gensim.models.keyedvectors import KeyedVectors\nfrom gensim.models.wrappers import FastText\nimport numpy as np\nfrom nltk.tokenize import TweetTokenizer\nimport itertools\nimport shutil\nimport hashlib\n\nrole = get_execution_role()\nbucket = 'sagemaker-lign167'# enter your s3 bucket where you will copy data and model artifacts\nprefix = 'finalproject' # place to upload training files within the bucket",
      "execution_count": 236,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Loading the mapping as well as processing the data\nVectorizing the words using GloVe and Word 2 Vec"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "def load_unicode_mapping(path):\n    emoji_dict = defaultdict()\n    with open(path, 'r', encoding='utf-8') as f:\n        lines = f.readlines()\n        for line in lines:\n            tokens = line.strip().split('\\t')\n            emoji_dict[tokens[0]] = tokens[1]\n    return emoji_dict",
      "execution_count": 238,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "def load_word2vec(path=None):\n    word2vecmodel = KeyedVectors.load_word2vec_format(path, binary=True)\n    return word2vecmodel",
      "execution_count": 239,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "def load_fasttext(path=None):\n    word2vecmodel = FastText.load_fasttext_format(path)\n    return word2vecmodel",
      "execution_count": 240,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "def InitializeWords(word_file_path):\n    word_dictionary = defaultdict()\n    \n    with open(word_file_path, 'r', encoding='utf-8') as f:\n        lines = f.readlines()\n        for line in lines:\n            tokens = line.lower().strip().split('\\t')\n            word_dictionary[tokens[0]] = int(tokens[1])\n            \n    for alphabet in \"bcdefghijklmnopqrstuvwxyz\":\n        if (alphabet in word_dictionary):\n                word_dictionary.__delitem__(alphabet)\n                \n    for word in ['ann', 'assis'\n                 'bz',\n                 'ch', 'cre', 'ct',\n                 'di',\n                 'ed', 'ee',\n                 'ic',\n                 'le',\n                 'ng', 'ns',\n                 'pr', 'picon',\n                 'th', 'tle', 'tl', 'tr',\n                 'um',\n                 've',\n                 'yi']:\n        if (word in word_dictionary):\n            word_dictionary.__delitem__(word)\n                \n    return word_dictionary",
      "execution_count": 241,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "def normalize_word(word):\n    temp = word\n    while True:\n        w = re.sub(r\"([a-zA-Z])\\1\\1\", r\"\\1\\1\", temp)\n        if (w == temp):\n            break\n        else:\n            temp = w\n    return w",
      "execution_count": 242,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "def load_split_word(split_word_file_path):\n    split_word_dictionary = defaultdict()\n    with open(split_word_file_path, 'r', encoding='utf-8') as f:\n        lines = f.readlines()\n        for line in lines:\n            tokens = line.lower().strip().split('\\t')\n            if (len(tokens) >= 2):\n                split_word_dictionary[tokens[0]] = tokens[1]\n    \n    print('split entry found:', len(split_word_dictionary.keys()))\n    return split_word_dictionary",
      "execution_count": 243,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "def split_hashtags(term, wordlist, split_word_list, dump_file=''):\n    if (len(term.strip()) == 1):\n        return ['']\n    \n    if (split_word_list != None and term.lower() in split_word_list):\n        return split_word_list.get(term.lower()).split(' ')\n    #else:\n        #print(term)\n        \n    if (term.startswith('#')):\n        term = term[1:]\n    \n    if (wordlist != None and term.lower() in wordlist):\n        return [term.lower()]\n    \n    words = []\n    penalty = -69971\n    max_coverage = penalty\n    \n    split_words_count = 6\n    \n    term = re.sub(r'([0-9]+)', r' \\1', term)\n    term = re.sub(r'(1st|2nd|3rd|4th|5th|6th|7th|8th|9th|0th)', r'\\1', term)\n    term = re.sub(r'([A-Z][^A-Z ]+)', r' \\1', term.strip())\n    term = re.sub(r'([A=Z]{2,})+', r' \\1', term)\n    words = term.strip().split(' ')\n    \n    n_splits = 0\n    \n    if (len(words) < 3):\n        chars = [c for c in term.lower()]\n        \n        found_all_words = False\n        \n        while (n_splits < split_words_count and not found_all_words):\n            for idx in itertools.combinations(range(0, len(chars)), n_splits):\n                output = np.split(chars, idx)\n                line = [''.join(o) for o in output]\n                \n                score = (1. / len(line)) *sum([wordlist.get( word.strip()) if word.strip() in wordlist else 0. \n                                               if word.strip().isnumeric() else penalty for word in line])\n                \n                if (score > max_coverage):\n                    words = line\n                    max_coverage = score\n                    \n                    line_is_valid_word = [word.strip() in wordlist if not word.isnumeric() else True for word in line]\n                    \n                    if (all(line_is_valid_word)):\n                        found_all_words = True\n            n_splits += 1\n    \n    with open(dump_file, 'a', encoding='utf-8') as f:\n        if (term != '' and len(words) > 0):\n            f.write('#'+str(term).strip() + '\\t' + ' '.join(words) + '\\t' + str(n_splits) + '\\n')\n        \n    return words",
      "execution_count": 244,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "def load_abbreviation(path='../data/twitter/abbreviations.txt'):\n    abbreviation_dict = defaultdict()\n    with open(path, 'r', encoding='utf-8') as f:\n        lines = f.readlines()\n        for line in lines:\n            token = line.lower().strip().split('\\t')\n            abbreviation_dict[token[0]] = token[1]\n        return abbreviation_dict",
      "execution_count": 245,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "def filter_text(text, word_list, split_word_list, emoji_dict, abbreviation_dict, dump_file_path,\n                normalize_text = False, split_hashtag = False,\n                ignore_profiles = False, replace_emoji= True):\n    filtered_text = []\n    \n    filter_list = ['/', '-', '=', '+', 'â€¦', '\\\\', '(', ')', '&', ':']\n    \n    for t in text:\n        word_tokens = None\n        \n        if (ignore_profiles and str(t).startswith(\"@\")):\n            continue\n            \n        if (str(t).startswith('http')):\n            continue\n            \n        if (str(t).lower() in ['#sarcasm']):\n            continue\n            \n        if (replace_emoji):\n            if (t in emoji_dict):\n                t = emoji_dict.get(t).split('_')\n                filtered_text.extend(t)\n                continue\n        if (split_hashtag and str(t).startswith(\"#\")):\n            splits = split_hashtags(t, word_list, split_word_list, \n                                    dump_file=dump_file_path)\n            if (splits != None):\n                filtered_text.extend([s for s in splits if (not filtered_text.__contains__(s))])\n                continue\n        \n        if (normalize_text):\n            t = normalize_word(t)\n            \n        if (t in abbreviation_dict):\n            tokens = abbreviation_dict.get(t).split(' ')\n            filtered_text.extend(tokens)\n            continue\n        \n        filtered_text.append(t)\n    \n    return filtered_text",
      "execution_count": 246,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "def parsedata(lines, word_list, split_word_list, emoji_dict, abbreviation_dict, dump_file_path,\n              normalize_text = False, split_hashtag = False, ignore_profiles = False,\n              lower_case = False, replace_emoji = True, n_grams = None, at_character = False,):\n    data = []\n    for i, line in enumerate(lines):\n        if (i % 10000 == 0):\n            print(str(i) + '...', end='', flush=True)\n        \n        try:\n            if (lower_case):\n                line = line.lower()\n            \n            token = line.split('\\t')\n            id = token[0]\n            \n            label = int(token[1].strip())\n            \n            target_text = TweetTokenizer().tokenize(token[2].strip())\n            if (at_character):\n                target_text = [c for c in token[2].strip()]\n                \n            if (n_grams != None):\n                n_grams_list = list(create_ngram_set(target_text, ngram_values=n_grams))\n                target_text.extend(['_'.join(n) for n in n_grams_list])\n                \n            target_text = filter_text(target_text, word_list, split_word_list, emoji_dict,\n                                      abbreviation_dict, dump_file_path, normalize_text, split_hashtag,\n                                      ignore_profiles, replace_emoji=replace_emoji)\n            \n            dimensions = []\n            if (len(token) > 3 and token[3].strip() != 'NA'):\n                dimensions = [dimension.split('@@')[1] for dimension in token[3].strip().split('|')]\n                \n            context = []\n            if (len(token) > 4):\n                if (token[4] != 'NA'):\n                    context = TweetTokenizer().tokenize(token[4].strip())\n                    context = filter_text(context, word_list, split_word_list, emoji_dict,\n                                         abbreviation_dict, dump_file_path, normalize_text, split_hashtag,\n                                         ignore_profiles, replace_emoji = replace_emoji)\n                    \n            author = 'NA'\n            if (len(token) > 5):\n                author = token[5]\n            \n            if (len(target_text) != 0):\n                data.append((id, label, target_text, dimensions, context, author))\n        except:\n            raise\n    print('')\n    return data",
      "execution_count": 247,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Loading Data"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "def loaddata(filename, word_file_path, split_word_path, emoji_file_path, dump_file_path, normalize_text=False,\n             split_hashtag=False, ignore_profiles=False, lower_case=True, replace_emoji=True,\n             n_grams=None, at_character=False):\n    word_list = None\n    emoji_dict = None\n    \n    split_word_list = load_split_word(split_word_path)\n    \n    if (split_hashtag):\n        word_list = InitializeWords(word_file_path)\n        \n    if (replace_emoji):\n        emoji_dict = load_unicode_mapping(emoji_file_path)\n        \n    abbreviation_dict = load_abbreviation()\n    \n    lines = open(filename, 'r', encoding='utf-8').readlines()\n    \n    data = parsedata(lines, word_list, split_word_list, emoji_dict, abbreviation_dict, dump_file_path, \n                     normalize_text=normalize_text, split_hashtag=split_hashtag,\n                     ignore_profiles=ignore_profiles, lower_case=lower_case, \n                     replace_emoji=replace_emoji, n_grams=n_grams, at_character=at_character)\n    return data",
      "execution_count": 248,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "def build_vocab(data, without_dimension=True, ignore_context=False, min_freq=0):\n    vocab = defaultdict(int)\n    vocab_freq = defaultdict(int)\n    \n    total_words = 1\n    if ( not without_dimension):\n        for i in range(1, 101):\n            vocab_freq[str(i)] = 0\n    for sentence_no, token in enumerate(data):\n        for word in token[2]:\n            if (word not in vocab_freq):\n                vocab_freq[word] = 0\n            vocab_freq[word] = vocab_freq.get(word) + 1\n        \n        if (not without_dimension):\n            for word in token[3]:\n                vocab_freq[word] = vocab_freq.get(word) + 1\n        \n        if (ignore_context == False):\n            for word in token[4]:\n                if (not word in vocab):\n                    vocab_freq[word] = 0\n                vocab_freq[word] = vocab_freq.get(word) + 1\n    \n    for k, v in vocab_freq.items():\n        if (v >= min_freq):\n            vocab[k] = total_words\n            total_words = total_words + 1\n    return vocab",
      "execution_count": 249,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "def build_reverse_vocab(vocab):\n    rev_vocab = defaultdict(str)\n    for k, v in vocab.items():\n        rev_vocab[v] = k\n    return rev_vocab",
      "execution_count": 250,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "def vectorize_word_dimension(data, vocab, drop_dimension_index = None):\n    X, Y, D, C, A = [], [], [], [], []\n    \n    known_words_set = set()\n    unknown_words_set = set()\n    \n    tokens = 0\n    token_coverage = 0\n    \n    for id, label, line, dimensions, context, author in data:\n        vec = []\n        context_vec = []\n        if (len(dimensions) != 0):\n            dvec = [vocab.get(d) for d in dimensions]\n        else:\n            dvec = [vocab.get('unk')] * 11\n        \n        if drop_dimension_index != None:\n            dvec.pop(drop_dimensions_index)\n            \n        for words in line:\n            tokens = tokens + 1\n            if (words in vocab):\n                vec.append(vocab[words])\n                token_coverage = token_coverage + 1\n                known_words_set.add(words)\n            else:\n                vec.append(vocab['unk'])\n                unknown_words_set.add(words)\n                \n        if (len(context) != 0):\n            for words in line:\n                tokens = tokens + 1\n                if (words in vocab):\n                    context_vec.append(vocab[words])\n                    token_coverage = token_coverage + 1\n                    known_words_set.add(words)\n                else:\n                    context_vec.append(vocab['unk'])\n                    unknown_words_set.add(words)\n        else:\n            context_vec = [vocab['unk']]\n            \n        X.append(vec)\n        Y.append(label)\n        D.append(dvec)\n        C.append(context_vec)\n        A.append(author)\n        \n    print('Token coverage:', token_coverage / float(tokens))\n    print('Word coverage:', len(known_words_set) / float(len(vocab.keys())))\n    return np.asarray(X), np.asarray(Y), np.asarray(D), np.asarray(C), np.asarray(A)",
      "execution_count": 251,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "def pad_sequence_1d(sequences, maxlen=None, dtype='float32',\n                    padding='pre', truncating='pre', value=0.):\n    X = [vectors for vectors in sequences]\n    \n    nb_samples = len(X)\n    \n    x = (np.zeros((nb_samples, maxlen)) * value).astype(dtype)\n    \n    for idx, s in enumerate(X):\n        if truncating == 'pre':\n            trunc = s[-maxlen:]\n        elif truncation == 'post':\n            trunc = s[:maxlen]\n        else:\n            raise ValueError(\"padding type '%s' not understood\" % padding)\n            \n        if padding == 'post':\n            x[idx, :len(trunc)] = trunc\n        elif padding == 'pre':\n            x[idx, -len(trunc):] = trunc\n        else:\n            raise ValueError(\"Padding type '%s' not understood\" % padding)\n        \n    return x",
      "execution_count": 252,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "def write_vocab(filepath, vocab):\n    with open(filepath, 'w', encoding='utf-8') as fw:\n        for key, value in vocab.items():\n            fw.write(str(key) + '\\t' + str(value) + '\\n')",
      "execution_count": 253,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "def get_fasttext_weight(vocab, n=300, path=None):\n    fasttextmodel = load_fasttext(path=path)\n    emb_weights = np.zeros((len(vocab.keys()) + 1, n))\n    for k, v in vocab.items():\n        if (fasttext.__contains__(k)):\n            emb_weights[v, :] = fasttext[k][:n]\n    \n    return emb_weights",
      "execution_count": 254,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "def get_word2vec_weight(vocab, n=300, path=None):\n    word2vecmodel = load_word2vec(path=path)\n    emb_weights = np.zeros((len(vocab.keys()) + 1, n))\n    for k, v in vocab.items():\n        if (word2vecmodel.__contains__(k)):\n            emb_weights[v, :] = word2vecmodel[k][:n]\n\n    return emb_weights",
      "execution_count": 255,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "def create_ngram_set(input_list, ngram_value=2):\n    \"\"\"\n    Extract a set of n-grams from a list of integers.\n    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)\n    {(4, 9), (4, 1), (1, 4), (9, 4)}\n    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)\n    [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]\n    \"\"\"\n    return set(zip(*[input_list[i:] for i in range(ngram_value)]))",
      "execution_count": 256,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "def prepend_line(infile, outfile, line):\n    with open(infile, 'r', encoding='utf-8') as old:\n        with open(outfile, 'w', encoding='utf-8') as new:\n            new.write(str(line) + \"\\n\")\n            shutil.copyfileobj(old, new)",
      "execution_count": 257,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "def prepend_slow(infile, outfile, line):\n    with open(infile, 'r', encoding='utf-8') as fin:\n        with open(outfile, 'w', encoding='utf-8') as fout:\n            fout.write(line + \"\\n\")\n            for line in fin:\n                fout.write(line)",
      "execution_count": 258,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "def checksum(filename):\n    BLOCKSIZE = 65536\n    hasher = hashlib.md5()\n    with open(filename, 'rb') as afile:\n        buf = afile.read(BLOCKSIZE)\n        while len(buf) > 0:\n            hasher.update(buf)\n            buf = afile.read(BLOCKSIZE)\n    return hasher.hexdigest()",
      "execution_count": 259,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "pretrain_num_lines = {\"glove.840B.300d.txt\": 2196017, \"glove.42B.300d.txt\": 1917494}\n\npretrain_checksum = {\n    \"glove.6B.300d.txt\": \"b78f53fb56ec1ce9edc367d2e6186ba4\",\n    \"glove.twitter.27B.50d.txt\": \"6e8369db39aa3ea5f7cf06c1f3745b06\",\n    \"glove.42B.300d.txt\": \"01fcdb413b93691a7a26180525a12d6e\",\n    \"glove.6B.50d.txt\": \"0fac3659c38a4c0e9432fe603de60b12\",\n    \"glove.6B.100d.txt\": \"dd7f3ad906768166883176d69cc028de\",\n    \"glove.twitter.27B.25d.txt\": \"f38598c6654cba5e6d0cef9bb833bdb1\",\n    \"glove.6B.200d.txt\": \"49fa83e4a287c42c6921f296a458eb80\",\n    \"glove.840B.300d.txt\": \"eec7d467bccfa914726b51aac484d43a\",\n    \"glove.twitter.27B.100d.txt\": \"ccbdddec6b9610196dd2e187635fee63\",\n    \"glove.twitter.27B.200d.txt\": \"e44cdc3e10806b5137055eeb08850569\",\n}\n\ndef check_num_lines_in_glove(filename, check_checksum=False):\n    if check_checksum:\n        assert checksum(filename) == pretrain_checksum[filename]\n    if filename.startswith('glove.6B.'):\n        return 400000\n    elif filename.startswith('glove.twitter.27B.'):\n        return 1193514\n    else:\n        return pretrain_num_lines[filename]",
      "execution_count": 260,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "def load_glove_word2vec(filename):\n    # Input: GloVe Model File\n    # More models can be downloaded from http://nlp.stanford.edu/projects/glove/\n    # print(filename[filename.rfind('/')+1:])\n\n    glove_file = filename[filename.rfind('/')+1:]\n    _, _,tokens, dimensions, _ = glove_file.split('.')\n    num_lines = check_num_lines_in_glove(glove_file)\n    dims = int(dimensions[:-1])\n\n    # Output: Gensim Model text format.\n    gensim_file = '/data/twitter/glove/glove_model.txt'\n    gensim_first_line = \"{} {}\".format(num_lines, dims)\n\n    # Prepends the line.\n    if platform == \"linux\" or platform == \"linux2\":\n        prepend_line(filename, gensim_file, gensim_first_line)\n    else:\n        prepend_slow(filename, gensim_file, gensim_first_line)\n\n    # Demo: Loads the newly created glove_model.txt into gensim API.\n    model = gensim.models.Word2Vec.load_word2vec_format(gensim_file, binary=False)  # GloVe Model\n\n    return model",
      "execution_count": 261,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "def get_glove_model(vocab, n=200):\n    word2vecmodel = load_glove_word2vec('../data/twitter/glove/glove.twitter.27B/glove.twitter.27B.200d.txt')\n\n    emb_weights = np.zeros((len(vocab.keys()) + 1, n))\n    for k, v in vocab.items():\n        if (word2vecmodel.__contains__(k)):\n            emb_weights[v, :] = word2vecmodel[k][:n]\n\n    return emb_weights",
      "execution_count": 262,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Importing Datasets"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "    basepath = os.path.dirname(os.getcwd())\n    tw_train_file = basepath + '/data/twitter/train/Train_v1.txt'\n    tw_validation_file = basepath + '/data/twitter/Dev_v1.txt'\n    tw_test_file = basepath + '/data/twitter/test/Test_v1.txt'\n    tw_word_file_path = basepath + '/data/twitter/word_list_freq.txt'\n    tw_split_word_path = basepath + '/data/twitter/word_split.txt'\n    tw_emoji_file_path = basepath + '/data/twitter/emoji_unicode_names_final.txt'\n    comb_train_file = basepath + '/data/combinded/train/Train_v1.txt'\n    comb_validation_file = basepath + '/data/combinded/Dev_v1.txt'\n    comb_test_file = basepath + '/data/combinded/test/Test_v1.txt'\n    comb_word_file_path = basepath + '/data/combinded/word_list_freq.txt'\n    comb_split_word_path = basepath + '/data/combinded/word_split.txt'\n    comb_emoji_file_path = basepath + '/data/combinded/emoji_unicode_names_final.txt'\n\n    tw_lstm_output_file = basepath + '/models/twitter/LSTM/TestResults.txt'\n    tw_lstm_model_file = basepath + '/models/twitter/LSTM/weights/'\n    tw_lstm_vocab_file_path = basepath + '/models/twitter/LSTM/vocab_list.txt'\n    tw_cnn_output_file = basepath + '/models/twitter/CNN/TestResults.txt'\n    tw_cnn_model_file = basepath + '/models/twitter/CNN/weights/'\n    tw_cnn_vocab_file_path = basepath + '/models/twitter/CNN/vocab_list.txt'\n    tw_dnn_output_file = basepath + '/models/twitter/DNN/TestResults.txt'\n    tw_dnn_model_file = basepath + '/models/twitter/DNN/weights/'\n    tw_dnn_vocab_file_path = basepath + '/models/twitter/DNN/vocab_list.txt'\n    tw_hashtag_split_file_path = basepath + '/data/twitter/hashtag_split_dump.txt'\n    comb_lstm_output_file = basepath + '/models/combinded/LSTM/TestResults.txt'\n    comb_lstm_model_file = basepath + '/models/combinded/LSTM/weights/'\n    comb_lstm_vocab_file_path = basepath + '/models/combinded/LSTM/vocab_list.txt'\n    comb_cnn_output_file = basepath + '/models/combinded/CNN/TestResults.txt'\n    comb_cnn_model_file = basepath + '/models/combinded/CNN/weights/'\n    comb_cnn_vocab_file_path = basepath + '/models/combinded/CNN/vocab_list.txt'\n    comb_dnn_output_file = basepath + '/models/combinded/DNN/TestResults.txt'\n    comb_dnn_model_file = basepath + '/models/combinded/DNN/weights/'\n    comb_dnn_vocab_file_path = basepath + '/models/combinded/DNN/vocab_list.txt'\n    comb_hashtag_split_file_path = basepath + '/data/combinded/hashtag_split_dump.txt'   ",
      "execution_count": 263,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "np.random.seed(1337)\nfrom sklearn import metrics\nfrom keras.models import Sequential, model_from_json\nfrom keras.layers.core import Dropout, Dense, Activation, Flatten\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.recurrent import LSTM\nfrom keras.layers.convolutional import Convolution1D, MaxPooling1D,Convolution2D\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.callbacks import EarlyStopping\nfrom keras.optimizers import Adam, SGD\nfrom keras.utils import np_utils\nfrom collections import defaultdict\nimport collections\nimport time",
      "execution_count": 309,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Declaring the models"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "class sarcasm_model():\n    _train_file = None\n    _test_file = None\n    _tweet_file = None\n    _output_file = None\n    _model_file_path = None\n    _word_file_path = None\n    _split_word_file_path = None\n    _emoji_file_path = None\n    _vocab_file_path = None\n    _input_weight_file_path = None\n    _vocab = None\n    _line_maxlen = None\n    _dump_file_path = None\n    def __init__(self):\n        self._line_maxlen = 30\n        \n    def _build_CNN_LSTM_DNN_network(self, vocab_size, maxlen, embedding_dimension=256,\n                                    hidden_units=256, trainable=False):\n        print('Build model...')\n        model = Sequential()\n        \n        model.add(Embedding(vocab_size, embedding_dimension, input_length=maxlen,\n                           embeddings_initializer='glorot_normal'))\n        \n        model.add(Convolution1D(hidden_units, 3, kernel_initializer='he_normal', padding='valid',\n                               activation='sigmoid',input_shape=(1, maxlen)))\n        \n        model.add(Convolution1D(hidden_units, 3, kernel_initializer='he_normal', padding='valid',\n                               activation='sigmoid',input_shape=(1, maxlen - 2)))\n        model.add(LSTM(hidden_units, kernel_initializer='he_normal', activation='sigmoid', dropout=0.5,\n                       return_sequences=True))\n        \n        model.add(LSTM(hidden_units, kernel_initializer='he_normal', activation='sigmoid', dropout=0.5))\n        model.add(Dense(hidden_units, kernel_initializer='he_normal', activation='sigmoid'))\n        model.add(Dense(2))\n        model.add(Activation('softmax'))\n        adam = Adam(lr=0.0001)\n        model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n        print('No of parameter:', model.count_params())\n\n        print(model.summary())\n        return model\n    def _build_LSTM_network(self, vocab_size, maxlen, embedding_dimension=256, hidden_units=256,\n                            trainable=False):\n        print('Build model...')\n        model = Sequential()\n        model.add(Embedding(vocab_size, embedding_dimension, input_length=maxlen,\n                           embeddings_initializer='glorot_normal'))\n        model.add(LSTM(hidden_units, kernel_initializer='he_normal', activation='sigmoid',\n                       dropout=0.2, return_sequences=True))\n        model.add(LSTM(2, kernel_initializer='he_normal', activation='sigmoid'))\n        adam = Adam(lr=0.00001)\n        model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n        print('No of parameter:', model.count_params())\n\n        print(model.summary())\n        return model\n    def _build_CNN_network(self, vocab_size, maxlen, embedding_dimension=256, hidden_units=256,\n                           trainable=False):\n        print('Build model...')\n        model = Sequential()\n        model.add(Embedding(vocab_size, embedding_dimension, input_length=maxlen,\n                           embeddings_initializer='glorot_normal'))\n        \n        model.add(Convolution1D(hidden_units, 3, kernel_initializer='he_normal', padding='valid',\n                               activation='sigmoid',input_shape=(1, maxlen)))\n        model.add(MaxPooling1D(pool_size=3))\n        model.add(Dropout(0.25))\n        model.add(Dense(activation = 'softmax', units=2))\n        adam = Adam(lr=0.00001)\n        model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n        print('No of parameter:', model.count_params())\n\n        print(model.summary())\n        return model\n\n    \n    def _build_standalone_CNN_network(self, vocab_size, maxlen, embedding_dimension=256, hidden_units=256,\n                           trainable=False):\n        model = Sequential()\n        model.add(Embedding(vocab_size, embedding_dimension, input_length=maxlen,\n                           embeddings_initializer='glorot_normal'))\n        model.add(Convolution1D(hidden_units, 3, kernel_initializer='he_normal', padding='valid',\n                               activation='sigmoid',input_shape=(1, maxlen)))\n        model.add(Convolution1D(hidden_units, 3, kernel_initializer='he_normal', padding='valid',\n                               activation='sigmoid',input_shape=(1, maxlen - 2)))\n        model.add(Flatten())\n        model.add(Dense(2))\n        sgd = SGD(lr=0.00001)\n        model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n        print('No of parameter:', model.count_params())\n\n        print(model.summary())\n        return model",
      "execution_count": 336,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Train Model"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "class train_model(sarcasm_model):\n    train = None\n    validation = None\n    model_type = 'CNN'\n    print(\"Loading data...\")\n    \n    def __init__(self, model_type, train_file, validation_file, word_file_path, split_word_path, emoji_file_path,\n                 model_file, vocab_file, output_file, dump_file_path, input_weight_file_path=None):\n        sarcasm_model.__init__(self)\n        self.model_type = model_type\n        self._train_file = train_file\n        self._validation_file = validation_file\n        self._word_file_path = word_file_path\n        self._split_word_file_path = split_word_path\n        self._emoji_file_path = emoji_file_path\n        self._model_file = model_file\n        self._vocab_file_path = vocab_file\n        self._output_file = output_file\n        self._input_weight_file_path = input_weight_file_path\n        self._dump_file_path = dump_file_path\n        \n        self.load_train_validation_data()\n        \n        print(self._line_maxlen)\n        \n        self._vocab = build_vocab(self.train, min_freq=1)\n        if ('unk' not in self._vocab):\n            self._vocab['unk'] = len(self._vocab.keys()) + 1\n        \n        print(len(self._vocab.keys()) + 1)\n        print('unk::', self._vocab['unk'])\n        write_vocab(self._vocab_file_path, self._vocab)\n        \n        X, Y, D, C, A = vectorize_word_dimension(self.train, self._vocab)\n        X = pad_sequence_1d(X, maxlen=self._line_maxlen)\n        \n        vX, vY, vD, vC, vA = vectorize_word_dimension(self.validation, self._vocab)\n        vX = pad_sequence_1d(vX, maxlen=self._line_maxlen)\n        \n        dimension_size = 256\n        \n        ratio = self.calculate_label_ratio(Y)\n        ratio = [max(ratio.values()) / value for key, value in ratio.items()]\n        print('class ratio::', ratio)\n        \n        Y, vY = [np_utils.to_categorical(x) for x in (Y, vY)]\n\n        print('train_X', X.shape)\n        print('train_Y', Y.shape)\n        print('validation_X', vX.shape)\n        print('validation_Y', vY.shape)\n        \n        if model_type == 'CNN':\n            model = self._build_CNN_network(len(self._vocab.keys()) + 1, self._line_maxlen,\n                                            embedding_dimension=dimension_size, trainable=True)\n        elif model_type == 'LSTM':\n            model = self._build_LSTM_network(len(self._vocab.keys()) + 1, self._line_maxlen,\n                                             embedding_dimension=dimension_size, trainable=True)\n        elif model_type == 'DNN':\n            model = self._build_CNN_LSTM_DNN_network(len(self._vocab.keys()) + 1, self._line_maxlen,\n                                                     embedding_dimension=dimension_size, \n                                                     trainable=True)\n        elif model_type == 'CNN_1D':\n              model = self._build_standalone_CNN_network(len(self._vocab.keys()) + 1, self._line_maxlen,\n                                                     embedding_dimension=dimension_size, \n                                                     trainable=True)\n        open(self._model_file + 'model.json', 'w').write(model.to_json())\n        save_best = ModelCheckpoint(model_file + 'model.json.hdf5', save_best_only=True)\n        save_all = ModelCheckpoint(self._model_file + 'weights.{epoch:02d}__.hdf5',\n                                   save_best_only=False)\n        early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=1)\n\n        # training\n        model.fit(X, Y, batch_size=8, epochs=10, validation_data=(vX, vY), shuffle=True,\n                  callbacks=[save_best, save_all, early_stopping], class_weight=ratio)\n        \n    def load_train_validation_data(self):\n        self.train = loaddata(self._train_file, self._word_file_path, self._split_word_file_path,\n                                 self._emoji_file_path, self._dump_file_path, normalize_text=True,\n                                 split_hashtag=False,\n                                 ignore_profiles=False)\n        print('Training data loading finished...')\n\n        self.validation = loaddata(self._validation_file, self._word_file_path, self._split_word_file_path,\n                                      self._emoji_file_path, self._dump_file_path,\n                                      normalize_text=True,\n                                      split_hashtag=False,\n                                      ignore_profiles=False)\n        print('Validation data loading finished...')\n\n        if (self._test_file != None):\n            self.test = loaddata(self._test_file, self._word_file_path, self._split_word_file_path,\n                                        self._emoji_file_path, self._dump_file_path,\n                                        normalize_text=True,\n                                        split_hashtag=True,\n                                        ignore_profiles=True)\n    def get_maxlen(self):\n        return max(map(len, (x for _, x in self.train + self.validation)))\n\n    def write_vocab(self):\n        with open(self._vocab_file_path, 'w', encoding='utf-8') as fw:\n            for key, value in self._vocab.iteritems():\n                fw.write(str(key) + '\\t' + str(value) + '\\n')\n\n    def calculate_label_ratio(self, labels):\n        return collections.Counter(labels)\n",
      "execution_count": 337,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Loading data...\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Test Model "
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "class test_model(sarcasm_model):\n    test = None\n    model = None\n\n    \n    def __init__(self, model_file, word_file_path, split_word_path, emoji_file_path, vocab_file_path,\n                output_file, dump_file_path, input_weight_file_path=None):\n        print('Initializing...')\n        sarcasm_model.__init__(self)\n        \n        self._model_file_path = model_file\n        self._word_file_path = word_file_path\n        self._split_word_file_path = split_word_path\n        self._emoji_file_path = emoji_file_path\n        self._vocab_file_path = vocab_file_path\n        self._output_file = output_file\n        self._input_weight_file_path = input_weight_file_path\n        self._dump_file_path = dump_file_path\n        \n        print('test_maxlen', self._line_maxlen)\n    \n    def load_trained_model(self, model_file='model.json', weight_file='model.json.hdf5'):\n        start = time.time()\n        self.__load_model(self._model_file_path + model_file, self._model_file_path + weight_file)\n        end = time.time()\n        print('model loading time::', (end - start))\n\n    def __load_model(self, model_path, model_weight_path):\n        self.model = model_from_json(open(model_path, encoding='utf-8').read())\n        print('model loaded from file...')\n        self.model.load_weights(model_weight_path)\n        print('model weights loaded from file...')\n\n    def load_vocab(self):\n        vocab = defaultdict()\n        with open(self._vocab_file_path, 'r', encoding='utf-8') as f:\n            for line in f.readlines():\n                key, value = line.split('\\t')\n                vocab[key] = value\n        return vocab\n    \n    def predict(self, test_file, verbose=False):\n        try:\n            start = time.time()\n            self.test = loaddata(test_file, self._word_file_path, self._split_word_file_path, self._emoji_file_path,\n                                    self._dump_file_path, normalize_text=True, split_hashtag=True,\n                                    ignore_profiles=False)\n            end = time.time()\n            if (verbose == True):\n                print('test resource loading time::', (end - start))\n\n            self._vocab = self.load_vocab()\n            print('vocab loaded...')\n\n            start = time.time()\n            tX, tY, tD, tC, tA = vectorize_word_dimension(self.test, self._vocab)\n            tX = pad_sequence_1d(tX, maxlen=self._line_maxlen)\n            end = time.time()\n            if (verbose == True):\n                print('test resource preparation time::', (end - start))\n\n            self.__predict_model(tX, self.test)\n        except Exception as e:\n            print('Error:', e)\n    \n    def __predict_model(self, tX, test):\n        y = []\n        y_pred = []\n        prediction_probability = self.model.predict(tX, batch_size=1, verbose=1)\n        try:\n            fd = open(self._output_file + '.analysis', 'w', encoding='utf-8')\n            for i, (label) in enumerate(prediction_probability):\n                id = test[i][0]\n                gold_label = test[i][1]\n                words = test[i][2]\n                dimensions = test[i][3]\n                context = test[i][4]\n                author = test[i][5]\n\n                predicted = np.argmax(prediction_probability[i])\n\n                y.append(int(gold_label))\n                y_pred.append(predicted)\n\n                fd.write(str(label[0]) + '\\t' + str(label[1]) + '\\t'\n                         + str(gold_label) + '\\t'\n                         + str(predicted) + '\\t'\n                         + ' '.join(words))\n\n                fd.write('\\n')\n\n            print('accuracy::', metrics.accuracy_score(y, y_pred))\n            print('precision::', metrics.precision_score(y, y_pred, average='weighted'))\n            print('recall::', metrics.recall_score(y, y_pred, average='weighted'))\n            print('f_score::', metrics.f1_score(y, y_pred, average='weighted'))\n            print('f_score::', metrics.classification_report(y, y_pred))\n            fd.close()\n        except Exception as e:\n            print(e)\n            ",
      "execution_count": 338,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Twitter Dataset Evaluation"
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": false
      },
      "cell_type": "code",
      "source": "twitter_LSTM_tr = train_model('LSTM', tw_train_file, tw_validation_file, tw_word_file_path, \n                              tw_split_word_path, tw_emoji_file_path, tw_lstm_model_file,\n                              tw_lstm_vocab_file_path, tw_hashtag_split_file_path, tw_lstm_output_file)",
      "execution_count": 166,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "split entry found: 10942\n0...10000...20000...30000...\nTraining data loading finished...\nsplit entry found: 10942\n0...\nValidation data loading finished...\n30\n33892\nunk:: 33891\nToken coverage: 1.0\nWord coverage: 0.9999704936413797\nToken coverage: 0.9858098520170282\nWord coverage: 0.13006402879820603\nclass ratio:: [1.0, 1.151665945478148]\ntrain_X (39780, 30)\ntrain_Y (39780, 2)\nvalidation_X (1605, 30)\nvalidation_Y (1605, 2)\nBuild model...\nNo of parameter: 9203736\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_4 (Embedding)      (None, 30, 256)           8676352   \n_________________________________________________________________\nlstm_7 (LSTM)                (None, 30, 256)           525312    \n_________________________________________________________________\nlstm_8 (LSTM)                (None, 2)                 2072      \n=================================================================\nTotal params: 9,203,736\nTrainable params: 9,203,736\nNon-trainable params: 0\n_________________________________________________________________\nNone\nTrain on 39780 samples, validate on 1605 samples\nEpoch 1/10\n39780/39780 [==============================] - 515s 13ms/step - loss: 0.6868 - acc: 0.5456 - val_loss: 0.6403 - val_acc: 0.8760\nEpoch 2/10\n39780/39780 [==============================] - 513s 13ms/step - loss: 0.6396 - acc: 0.6270 - val_loss: 0.5708 - val_acc: 0.7053\nEpoch 3/10\n39780/39780 [==============================] - 512s 13ms/step - loss: 0.6018 - acc: 0.6694 - val_loss: 0.6230 - val_acc: 0.6685\nEpoch 4/10\n39780/39780 [==============================] - 512s 13ms/step - loss: 0.5875 - acc: 0.7016 - val_loss: 0.6619 - val_acc: 0.6305\nEpoch 5/10\n39780/39780 [==============================] - 512s 13ms/step - loss: 0.5755 - acc: 0.7185 - val_loss: 0.6274 - val_acc: 0.6860\nEpoch 6/10\n39780/39780 [==============================] - 512s 13ms/step - loss: 0.5583 - acc: 0.7234 - val_loss: 0.6226 - val_acc: 0.6280\nEpoch 7/10\n39780/39780 [==============================] - 512s 13ms/step - loss: 0.5613 - acc: 0.7328 - val_loss: 0.6193 - val_acc: 0.6903\nEpoch 8/10\n39780/39780 [==============================] - 512s 13ms/step - loss: 0.5507 - acc: 0.7490 - val_loss: 0.7271 - val_acc: 0.6449\nEpoch 9/10\n39780/39780 [==============================] - 512s 13ms/step - loss: 0.5428 - acc: 0.7571 - val_loss: 0.6397 - val_acc: 0.6611\nEpoch 10/10\n39780/39780 [==============================] - 513s 13ms/step - loss: 0.5346 - acc: 0.7609 - val_loss: 0.7187 - val_acc: 0.6617\n"
        }
      ]
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": false
      },
      "cell_type": "code",
      "source": "twitter_LSTM_te = test_model(tw_lstm_model_file, tw_word_file_path, tw_split_word_path,\n                             tw_emoji_file_path, tw_lstm_vocab_file_path, tw_hashtag_split_file_path, tw_lstm_output_file)\ntwitter_LSTM_te.load_trained_model(weight_file='weights.07__.hdf5')\ntwitter_LSTM_te.predict(tw_test_file)",
      "execution_count": 170,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Initializing...\ntest_maxlen 30\nmodel loaded from file...\nmodel weights loaded from file...\nmodel loading time:: 1.713092565536499\nsplit entry found: 10942\n0...\nvocab loaded...\nToken coverage: 0.9882122357552849\nWord coverage: 0.12487090968103626\n2000/2000 [==============================] - 67s 34ms/step\naccuracy:: 0.772\nprecision:: 0.7721088435374149\nrecall:: 0.772\nf_score:: 0.771977197719772\nf_score::              precision    recall  f1-score   support\n\n          0       0.77      0.78      0.77      1000\n          1       0.78      0.76      0.77      1000\n\navg / total       0.77      0.77      0.77      2000\n\n"
        }
      ]
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# twitter_CNN_tr = train_model('CNN_1D', tw_train_file, tw_validation_file, tw_word_file_path, \n#                               tw_split_word_path, tw_emoji_file_path, tw_cnn_model_file,\n#                               tw_cnn_vocab_file_path, tw_hashtag_split_file_path, tw_cnn_output_file)",
      "execution_count": 334,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "twitter_CNN_te = test_model(tw_cnn_model_file, tw_word_file_path, tw_split_word_path,\n                             tw_emoji_file_path, tw_cnn_vocab_file_path, tw_hashtag_split_file_path, tw_cnn_output_file)\ntwitter_CNN_te.load_trained_model(weight_file='weights.05__.hdf5')\ntwitter_CNN_te.predict(tw_test_file)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": false
      },
      "cell_type": "code",
      "source": "twitter_DNN_tr = train_model('DNN', tw_train_file, tw_validation_file, tw_word_file_path, \n                              tw_split_word_path, tw_emoji_file_path, tw_dnn_model_file,\n                              tw_dnn_vocab_file_path, tw_hashtag_split_file_path, tw_dnn_output_file)",
      "execution_count": 208,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "split entry found: 10942\n0...10000...20000...30000...\nTraining data loading finished...\nsplit entry found: 10942\n0...\nValidation data loading finished...\n30\n33892\nunk:: 33891\nToken coverage: 1.0\nWord coverage: 0.9999704936413797\nToken coverage: 0.9858098520170282\nWord coverage: 0.13006402879820603\nclass ratio:: [1.0, 1.151665945478148]\ntrain_X (39780, 30)\ntrain_Y (39780, 2)\nvalidation_X (1605, 30)\nvalidation_Y (1605, 2)\nBuild model...\nNo of parameter: 10187010\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_15 (Embedding)     (None, 30, 256)           8676352   \n_________________________________________________________________\nconv1d_11 (Conv1D)           (None, 28, 256)           196864    \n_________________________________________________________________\nconv1d_12 (Conv1D)           (None, 26, 256)           196864    \n_________________________________________________________________\nlstm_9 (LSTM)                (None, 26, 256)           525312    \n_________________________________________________________________\nlstm_10 (LSTM)               (None, 256)               525312    \n_________________________________________________________________\ndense_21 (Dense)             (None, 256)               65792     \n_________________________________________________________________\ndense_22 (Dense)             (None, 2)                 514       \n_________________________________________________________________\nactivation_1 (Activation)    (None, 2)                 0         \n=================================================================\nTotal params: 10,187,010\nTrainable params: 10,187,010\nNon-trainable params: 0\n_________________________________________________________________\nNone\nTrain on 39780 samples, validate on 1605 samples\nEpoch 1/10\n39780/39780 [==============================] - 514s 13ms/step - loss: 0.6660 - acc: 0.5739 - val_loss: 0.6811 - val_acc: 0.5994\nEpoch 2/10\n39780/39780 [==============================] - 510s 13ms/step - loss: 0.5011 - acc: 0.7575 - val_loss: 0.8326 - val_acc: 0.5427\nEpoch 3/10\n39780/39780 [==============================] - 510s 13ms/step - loss: 0.4069 - acc: 0.8192 - val_loss: 0.8717 - val_acc: 0.5483\nEpoch 4/10\n39780/39780 [==============================] - 510s 13ms/step - loss: 0.3569 - acc: 0.8466 - val_loss: 0.8368 - val_acc: 0.6093\nEpoch 5/10\n39780/39780 [==============================] - 510s 13ms/step - loss: 0.3183 - acc: 0.8681 - val_loss: 0.7914 - val_acc: 0.6293\nEpoch 6/10\n39780/39780 [==============================] - 510s 13ms/step - loss: 0.2908 - acc: 0.8809 - val_loss: 1.0127 - val_acc: 0.5794\nEpoch 7/10\n39780/39780 [==============================] - 510s 13ms/step - loss: 0.2690 - acc: 0.8901 - val_loss: 0.8135 - val_acc: 0.6330\nEpoch 8/10\n39780/39780 [==============================] - 510s 13ms/step - loss: 0.2475 - acc: 0.9020 - val_loss: 1.0793 - val_acc: 0.5844\nEpoch 9/10\n39780/39780 [==============================] - 510s 13ms/step - loss: 0.2338 - acc: 0.9080 - val_loss: 0.8001 - val_acc: 0.6766\nEpoch 10/10\n39780/39780 [==============================] - 510s 13ms/step - loss: 0.2200 - acc: 0.9134 - val_loss: 0.9277 - val_acc: 0.6417\n"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "twitter_DNN_te = test_model(tw_dnn_model_file, tw_word_file_path, tw_split_word_path,\n                             tw_emoji_file_path, tw_dnn_vocab_file_path, tw_hashtag_split_file_path, tw_dnn_output_file)\ntwitter_DNN_te.load_trained_model(weight_file='weights.09__.hdf5')\ntwitter_DNN_te.predict(tw_test_file)",
      "execution_count": 209,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Initializing...\ntest_maxlen 30\nmodel loaded from file...\nmodel weights loaded from file...\nmodel loading time:: 2.7530746459960938\nsplit entry found: 10942\n0...\nvocab loaded...\nToken coverage: 0.9882122357552849\nWord coverage: 0.12487090968103626\n2000/2000 [==============================] - 68s 34ms/step\naccuracy:: 0.8825\nprecision:: 0.8855539730202938\nrecall:: 0.8825\nf_score:: 0.8822668589474306\nf_score::              precision    recall  f1-score   support\n\n          0       0.92      0.84      0.88      1000\n          1       0.85      0.93      0.89      1000\n\navg / total       0.89      0.88      0.88      2000\n\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## The Start of the Combined Dataset"
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": false
      },
      "cell_type": "code",
      "source": "comb_LSTM_tr = train_model('LSTM', comb_train_file, comb_validation_file, comb_word_file_path, \n                              comb_split_word_path, comb_emoji_file_path, comb_lstm_model_file,\n                              comb_lstm_vocab_file_path, comb_hashtag_split_file_path, comb_lstm_output_file)",
      "execution_count": 347,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "split entry found: 10942\n0...10000...20000...30000...40000...50000...60000...70000...80000...\nTraining data loading finished...\nsplit entry found: 10942\n0...\nValidation data loading finished...\n30\n56946\nunk:: 56945\nToken coverage: 1.0\nWord coverage: 0.9999824391957152\nToken coverage: 0.9737100635109518\nWord coverage: 0.1363772060760383\nclass ratio:: [1.08667772523582, 1.0]\ntrain_X (84283, 30)\ntrain_Y (84283, 2)\nvalidation_X (4681, 30)\nvalidation_Y (4681, 2)\nBuild model...\nNo of parameter: 15105560\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_43 (Embedding)     (None, 30, 256)           14578176  \n_________________________________________________________________\nlstm_15 (LSTM)               (None, 30, 256)           525312    \n_________________________________________________________________\nlstm_16 (LSTM)               (None, 2)                 2072      \n=================================================================\nTotal params: 15,105,560\nTrainable params: 15,105,560\nNon-trainable params: 0\n_________________________________________________________________\nNone\nTrain on 84283 samples, validate on 4681 samples\nEpoch 1/10\n84283/84283 [==============================] - 1180s 14ms/step - loss: 0.6871 - acc: 0.5459 - val_loss: 0.6686 - val_acc: 0.5824\nEpoch 2/10\n84283/84283 [==============================] - 1172s 14ms/step - loss: 0.6481 - acc: 0.6131 - val_loss: 0.6395 - val_acc: 0.6206\nEpoch 3/10\n84283/84283 [==============================] - 1174s 14ms/step - loss: 0.6233 - acc: 0.6548 - val_loss: 0.6325 - val_acc: 0.6407\nEpoch 4/10\n84283/84283 [==============================] - 1172s 14ms/step - loss: 0.6061 - acc: 0.6824 - val_loss: 0.6290 - val_acc: 0.6529\nEpoch 5/10\n84283/84283 [==============================] - 1174s 14ms/step - loss: 0.5930 - acc: 0.6996 - val_loss: 0.6357 - val_acc: 0.6413\nEpoch 6/10\n84283/84283 [==============================] - 1174s 14ms/step - loss: 0.5824 - acc: 0.7152 - val_loss: 0.7218 - val_acc: 0.6567\nEpoch 7/10\n84283/84283 [==============================] - 1174s 14ms/step - loss: 0.5719 - acc: 0.7228 - val_loss: 0.6371 - val_acc: 0.6640\nEpoch 8/10\n84283/84283 [==============================] - 1174s 14ms/step - loss: 0.5600 - acc: 0.7304 - val_loss: 0.6423 - val_acc: 0.6674\nEpoch 9/10\n84283/84283 [==============================] - 1173s 14ms/step - loss: 0.5543 - acc: 0.7358 - val_loss: 0.6758 - val_acc: 0.6642\nEpoch 10/10\n84283/84283 [==============================] - 1173s 14ms/step - loss: 0.5480 - acc: 0.7486 - val_loss: 0.6825 - val_acc: 0.6755\n"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "comb_LSTM_te = test_model(comb_lstm_model_file, comb_word_file_path, comb_split_word_path,\n                             comb_emoji_file_path, comb_lstm_vocab_file_path, comb_hashtag_split_file_path, comb_lstm_output_file)\ncomb_LSTM_te.load_trained_model(weight_file='weights.10__.hdf5')\ncomb_LSTM_te.predict(comb_test_file)",
      "execution_count": 348,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Initializing...\ntest_maxlen 30\nmodel loaded from file...\nmodel weights loaded from file...\nmodel loading time:: 4.124856472015381\nsplit entry found: 10942\n0...\nvocab loaded...\nToken coverage: 0.9754688361831219\nWord coverage: 0.13620159803318993\n4701/4701 [==============================] - 162s 34ms/step\naccuracy:: 0.6432673899170389\nprecision:: 0.6429775830002576\nrecall:: 0.6432673899170389\nf_score:: 0.6399590314882202\nf_score::              precision    recall  f1-score   support\n\n          0       0.64      0.73      0.68      2495\n          1       0.64      0.55      0.59      2206\n\navg / total       0.64      0.64      0.64      4701\n\n"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "comb_CNN_tr = train_model('CNN_1D', comb_train_file, comb_validation_file, comb_word_file_path, \n                              comb_split_word_path, comb_emoji_file_path, comb_cnn_model_file,\n                              comb_cnn_vocab_file_path, comb_hashtag_split_file_path, comb_cnn_output_file)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "comb_CNN_te = test_model(comb_cnn_model_file, comb_word_file_path, comb_split_word_path,\n                             comb_emoji_file_path, comb_cnn_vocab_file_path, comb_hashtag_split_file_path, comb_cnn_output_file)\ncomb_CNN_te.load_trained_model(weight_file='weights.05__.hdf5')\ncomb_CNN_te.predict(comb_test_file)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "comb_DNN_tr = train_model('DNN', comb_train_file, comb_validation_file, comb_word_file_path, \n                              comb_split_word_path, comb_emoji_file_path, comb_dnn_model_file,\n                              comb_dnn_vocab_file_path, comb_hashtag_split_file_path, comb_dnn_output_file)",
      "execution_count": 350,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "split entry found: 10942\n0...10000...20000...30000...40000...50000...60000...70000...80000...\nTraining data loading finished...\nsplit entry found: 10942\n0...\nValidation data loading finished...\n30\n56946\nunk:: 56945\nToken coverage: 1.0\nWord coverage: 0.9999824391957152\nToken coverage: 0.9737100635109518\nWord coverage: 0.1363772060760383\nclass ratio:: [1.08667772523582, 1.0]\ntrain_X (84283, 30)\ntrain_Y (84283, 2)\nvalidation_X (4681, 30)\nvalidation_Y (4681, 2)\nBuild model...\nNo of parameter: 16088834\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_45 (Embedding)     (None, 30, 256)           14578176  \n_________________________________________________________________\nconv1d_57 (Conv1D)           (None, 28, 256)           196864    \n_________________________________________________________________\nconv1d_58 (Conv1D)           (None, 26, 256)           196864    \n_________________________________________________________________\nlstm_19 (LSTM)               (None, 26, 256)           525312    \n_________________________________________________________________\nlstm_20 (LSTM)               (None, 256)               525312    \n_________________________________________________________________\ndense_61 (Dense)             (None, 256)               65792     \n_________________________________________________________________\ndense_62 (Dense)             (None, 2)                 514       \n_________________________________________________________________\nactivation_10 (Activation)   (None, 2)                 0         \n=================================================================\nTotal params: 16,088,834\nTrainable params: 16,088,834\nNon-trainable params: 0\n_________________________________________________________________\nNone\nTrain on 84283 samples, validate on 4681 samples\nEpoch 1/10\n84283/84283 [==============================] - 1561s 19ms/step - loss: 0.6677 - acc: 0.5683 - val_loss: 0.6038 - val_acc: 0.6672\nEpoch 2/10\n84283/84283 [==============================] - 1556s 18ms/step - loss: 0.5558 - acc: 0.7136 - val_loss: 0.6246 - val_acc: 0.6597\nEpoch 3/10\n84283/84283 [==============================] - 1555s 18ms/step - loss: 0.4949 - acc: 0.7593 - val_loss: 0.5818 - val_acc: 0.6969\nEpoch 4/10\n84283/84283 [==============================] - 1553s 18ms/step - loss: 0.4521 - acc: 0.7878 - val_loss: 0.5760 - val_acc: 0.7048\nEpoch 5/10\n84283/84283 [==============================] - 1555s 18ms/step - loss: 0.4150 - acc: 0.8102 - val_loss: 0.6140 - val_acc: 0.6832\nEpoch 6/10\n84283/84283 [==============================] - 1554s 18ms/step - loss: 0.3776 - acc: 0.8316 - val_loss: 0.6229 - val_acc: 0.7018\nEpoch 7/10\n84283/84283 [==============================] - 1557s 18ms/step - loss: 0.3414 - acc: 0.8521 - val_loss: 0.6914 - val_acc: 0.6994\nEpoch 8/10\n84283/84283 [==============================] - 1554s 18ms/step - loss: 0.3121 - acc: 0.8661 - val_loss: 0.6702 - val_acc: 0.6890\nEpoch 9/10\n84283/84283 [==============================] - 1556s 18ms/step - loss: 0.2870 - acc: 0.8788 - val_loss: 0.7152 - val_acc: 0.6821\nEpoch 10/10\n84283/84283 [==============================] - 1557s 18ms/step - loss: 0.2663 - acc: 0.8879 - val_loss: 0.7267 - val_acc: 0.6830\n"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "comb_DNN_te = test_model(comb_dnn_model_file, comb_word_file_path, comb_split_word_path,\n                             comb_emoji_file_path, comb_dnn_vocab_file_path, comb_hashtag_split_file_path, comb_dnn_output_file)\ncomb_DNN_te.load_trained_model(weight_file='weights.06__.hdf5')\ncomb_DNN_te.predict(comb_test_file)",
      "execution_count": 354,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Initializing...\ntest_maxlen 30\nmodel loaded from file...\nmodel weights loaded from file...\nmodel loading time:: 5.138683319091797\nsplit entry found: 10942\n0...\nvocab loaded...\nToken coverage: 0.9754688361831219\nWord coverage: 0.13620159803318993\n4701/4701 [==============================] - 162s 34ms/step\naccuracy:: 0.6773027015528611\nprecision:: 0.6770971219720416\nrecall:: 0.6773027015528611\nf_score:: 0.6771775510849605\nf_score::              precision    recall  f1-score   support\n\n          0       0.69      0.70      0.70      2495\n          1       0.66      0.65      0.65      2206\n\navg / total       0.68      0.68      0.68      4701\n\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Reddit Data Evaluation"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "red_train_file = basepath + '/data/reddit/train/Train_v1.txt'\nred_validation_file = basepath + '/data/reddit/Dev_v1.txt'\nred_test_file = basepath + '/data/reddit/test/Test_v1.txt'\nred_word_file_path = basepath + '/data/reddit/word_list_freq.txt'\nred_split_word_path = basepath + '/data/reddit/word_split.txt'\nred_emoji_file_path = basepath + '/data/reddit/emoji_unicode_names_final.txt'\n\nred_lstm_output_file = basepath + '/models/reddit/LSTM/TestResults.txt'\nred_lstm_model_file = basepath + '/models/reddit/LSTM/weights/'\nred_lstm_vocab_file_path = basepath + '/models/reddit/LSTM/vocab_list.txt'\nred_cnn_output_file = basepath + '/models/reddit/CNN/TestResults.txt'\nred_cnn_model_file = basepath + '/models/reddit/CNN/weights/'\nred_cnn_vocab_file_path = basepath + '/models/reddit/CNN/vocab_list.txt'\nred_dnn_output_file = basepath + '/models/reddit/DNN/TestResults.txt'\nred_dnn_model_file = basepath + '/models/reddit/DNN/weights/'\nred_dnn_vocab_file_path = basepath + '/models/reddit/DNN/vocab_list.txt'\nred_hashtag_split_file_path = basepath + '/data/reddit/hashtag_split_dump.txt'",
      "execution_count": 352,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "red_LSTM_tr = train_model('LSTM', red_train_file, red_validation_file, red_word_file_path, \n                              red_split_word_path, red_emoji_file_path, red_lstm_model_file,\n                              red_lstm_vocab_file_path, red_hashtag_split_file_path, red_lstm_output_file)",
      "execution_count": 355,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "split entry found: 10942\n0...10000...20000...30000...40000...\nTraining data loading finished...\nsplit entry found: 10942\n0...\nValidation data loading finished...\n30\n34074\nunk:: 34073\nToken coverage: 1.0\nWord coverage: 0.9999706512487894\nToken coverage: 0.9657298157231871\nWord coverage: 0.142546884630059\nclass ratio:: [1.005113961997972, 1.0]\ntrain_X (45482, 30)\ntrain_Y (45482, 2)\nvalidation_X (2527, 30)\nvalidation_Y (2527, 2)\nBuild model...\nNo of parameter: 9250328\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_46 (Embedding)     (None, 30, 256)           8722944   \n_________________________________________________________________\nlstm_21 (LSTM)               (None, 30, 256)           525312    \n_________________________________________________________________\nlstm_22 (LSTM)               (None, 2)                 2072      \n=================================================================\nTotal params: 9,250,328\nTrainable params: 9,250,328\nNon-trainable params: 0\n_________________________________________________________________\nNone\nTrain on 45482 samples, validate on 2527 samples\nEpoch 1/10\n45482/45482 [==============================] - 740s 16ms/step - loss: 0.6909 - acc: 0.5321 - val_loss: 0.6857 - val_acc: 0.5983\nEpoch 2/10\n45482/45482 [==============================] - 733s 16ms/step - loss: 0.6731 - acc: 0.5968 - val_loss: 0.6670 - val_acc: 0.6110\nEpoch 3/10\n45482/45482 [==============================] - 733s 16ms/step - loss: 0.6532 - acc: 0.6127 - val_loss: 0.6599 - val_acc: 0.6098\nEpoch 4/10\n45482/45482 [==============================] - 733s 16ms/step - loss: 0.6366 - acc: 0.6304 - val_loss: 0.6450 - val_acc: 0.6264\nEpoch 5/10\n45482/45482 [==============================] - 733s 16ms/step - loss: 0.6196 - acc: 0.6545 - val_loss: 0.6407 - val_acc: 0.6146\nEpoch 6/10\n45482/45482 [==============================] - 733s 16ms/step - loss: 0.6060 - acc: 0.6786 - val_loss: 0.6402 - val_acc: 0.6399\nEpoch 7/10\n45482/45482 [==============================] - 732s 16ms/step - loss: 0.6016 - acc: 0.6976 - val_loss: 0.6343 - val_acc: 0.6474\nEpoch 8/10\n45482/45482 [==============================] - 733s 16ms/step - loss: 0.5890 - acc: 0.7087 - val_loss: 0.6292 - val_acc: 0.6450\nEpoch 9/10\n45482/45482 [==============================] - 733s 16ms/step - loss: 0.5840 - acc: 0.7048 - val_loss: 0.6480 - val_acc: 0.6474\nEpoch 10/10\n45482/45482 [==============================] - 732s 16ms/step - loss: 0.5743 - acc: 0.7199 - val_loss: 0.6553 - val_acc: 0.6514\n"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "red_LSTM_te = test_model(red_lstm_model_file, red_word_file_path, red_split_word_path,\n                             red_emoji_file_path, red_lstm_vocab_file_path, red_hashtag_split_file_path, red_lstm_output_file)\nred_LSTM_te.load_trained_model(weight_file='weights.10__.hdf5')\nred_LSTM_te.predict(red_test_file)",
      "execution_count": 356,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Initializing...\ntest_maxlen 30\nmodel loaded from file...\nmodel weights loaded from file...\nmodel loading time:: 5.097896099090576\nsplit entry found: 10942\n0...\nvocab loaded...\nToken coverage: 0.9677645186953063\nWord coverage: 0.14134358583042292\n2527/2527 [==============================] - 89s 35ms/step\naccuracy:: 0.6632370399683419\nprecision:: 0.6673562246543437\nrecall:: 0.6632370399683419\nf_score:: 0.659818574484986\nf_score::              precision    recall  f1-score   support\n\n          0       0.64      0.76      0.70      1291\n          1       0.69      0.56      0.62      1236\n\navg / total       0.67      0.66      0.66      2527\n\n"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "red_CNN_tr = train_model('CNN_1D', red_train_file, red_validation_file, red_word_file_path, \n                              red_split_word_path, red_emoji_file_path, red_cnn_model_file,\n                              red_cnn_vocab_file_path, red_hashtag_split_file_path, red_cnn_output_file)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "red_CNN_te = test_model(red_cnn_model_file, red_word_file_path, red_split_word_path,\n                             red_emoji_file_path, red_cnn_vocab_file_path, red_hashtag_split_file_path, red_cnn_output_file)\nred_CNN_te.load_trained_model(weight_file='weights.05__.hdf5')\nred_CNN_te.predict(red_test_file)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "red_DNN_tr = train_model('DNN', red_train_file, red_validation_file, red_word_file_path, \n                              red_split_word_path, red_emoji_file_path, red_dnn_model_file,\n                              red_dnn_vocab_file_path, red_hashtag_split_file_path, red_dnn_output_file)",
      "execution_count": 357,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "split entry found: 10942\n0...10000...20000...30000...40000...\nTraining data loading finished...\nsplit entry found: 10942\n0...\nValidation data loading finished...\n30\n34074\nunk:: 34073\nToken coverage: 1.0\nWord coverage: 0.9999706512487894\nToken coverage: 0.9657298157231871\nWord coverage: 0.142546884630059\nclass ratio:: [1.005113961997972, 1.0]\ntrain_X (45482, 30)\ntrain_Y (45482, 2)\nvalidation_X (2527, 30)\nvalidation_Y (2527, 2)\nBuild model...\nNo of parameter: 10233602\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_47 (Embedding)     (None, 30, 256)           8722944   \n_________________________________________________________________\nconv1d_59 (Conv1D)           (None, 28, 256)           196864    \n_________________________________________________________________\nconv1d_60 (Conv1D)           (None, 26, 256)           196864    \n_________________________________________________________________\nlstm_23 (LSTM)               (None, 26, 256)           525312    \n_________________________________________________________________\nlstm_24 (LSTM)               (None, 256)               525312    \n_________________________________________________________________\ndense_63 (Dense)             (None, 256)               65792     \n_________________________________________________________________\ndense_64 (Dense)             (None, 2)                 514       \n_________________________________________________________________\nactivation_11 (Activation)   (None, 2)                 0         \n=================================================================\nTotal params: 10,233,602\nTrainable params: 10,233,602\nNon-trainable params: 0\n_________________________________________________________________\nNone\nTrain on 45482 samples, validate on 2527 samples\nEpoch 1/10\n45482/45482 [==============================] - 815s 18ms/step - loss: 0.6820 - acc: 0.5432 - val_loss: 0.6419 - val_acc: 0.6340\nEpoch 2/10\n45482/45482 [==============================] - 806s 18ms/step - loss: 0.5970 - acc: 0.6825 - val_loss: 0.6081 - val_acc: 0.6708\nEpoch 3/10\n45482/45482 [==============================] - 806s 18ms/step - loss: 0.5250 - acc: 0.7428 - val_loss: 0.6176 - val_acc: 0.6810\nEpoch 4/10\n45482/45482 [==============================] - 806s 18ms/step - loss: 0.4647 - acc: 0.7822 - val_loss: 0.6716 - val_acc: 0.6514\nEpoch 5/10\n45482/45482 [==============================] - 804s 18ms/step - loss: 0.4141 - acc: 0.8137 - val_loss: 0.6654 - val_acc: 0.6632\nEpoch 6/10\n45482/45482 [==============================] - 805s 18ms/step - loss: 0.3715 - acc: 0.8388 - val_loss: 0.7119 - val_acc: 0.6700\nEpoch 7/10\n45482/45482 [==============================] - 807s 18ms/step - loss: 0.3348 - acc: 0.8564 - val_loss: 0.7625 - val_acc: 0.6636\nEpoch 8/10\n45482/45482 [==============================] - 805s 18ms/step - loss: 0.3069 - acc: 0.8698 - val_loss: 0.8237 - val_acc: 0.6672\nEpoch 9/10\n45482/45482 [==============================] - 808s 18ms/step - loss: 0.2828 - acc: 0.8825 - val_loss: 0.7917 - val_acc: 0.6609\nEpoch 10/10\n45482/45482 [==============================] - 804s 18ms/step - loss: 0.2648 - acc: 0.8902 - val_loss: 0.8330 - val_acc: 0.6581\n"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "red_DNN_te = test_model(red_dnn_model_file, red_word_file_path, red_split_word_path,\n                             red_emoji_file_path, red_dnn_vocab_file_path, red_hashtag_split_file_path, red_dnn_output_file)\nred_DNN_te.load_trained_model(weight_file='weights.06__.hdf5')\nred_DNN_te.predict(red_test_file)",
      "execution_count": 358,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Initializing...\ntest_maxlen 30\nmodel loaded from file...\nmodel weights loaded from file...\nmodel loading time:: 5.716418743133545\nsplit entry found: 10942\n0...\nvocab loaded...\nToken coverage: 0.9677645186953063\nWord coverage: 0.14134358583042292\n2527/2527 [==============================] - 88s 35ms/step\naccuracy:: 0.6640284922833399\nprecision:: 0.664410511188802\nrecall:: 0.6640284922833399\nf_score:: 0.6640613240282733\nf_score::              precision    recall  f1-score   support\n\n          0       0.68      0.66      0.67      1291\n          1       0.65      0.67      0.66      1236\n\navg / total       0.66      0.66      0.66      2527\n\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Combining Data"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "tw_tr = pd.read_table(basepath + '/data/twitter/train/Train_v1.txt', encoding='utf-8', header=None)\ntw_te = pd.read_table(basepath + '/data/twitter/test/Test_v1.txt', encoding='utf-8', header=None)\ntw_va = pd.read_table(basepath + '/data/twitter/Dev_v1.txt', encoding='utf-8', header=None)\ntw_tr = tw_tr.sample(frac=1)\ntw_te = tw_te.sample(frac=1)\ntw_va = tw_va.sample(frac=1)\ntw_va.reset_index(drop=True, inplace=True)\ntw_tr.reset_index(drop=True, inplace=True)\ntw_te.reset_index(drop=True, inplace=True)\ntw_te.to_csv(basepath + '/data/twitter/test/Test_v1.txt', header=False, index=False, sep='\\t', encoding='utf-8')\ntw_va.to_csv(basepath + '/data/twitter/Dev_v1.txt', header=False, index=False, sep='\\t', encoding='utf-8')\ntw_tr.to_csv(basepath + '/data/twitter/train/Train_v1.txt', index=False ,sep='\\t', header=False, encoding='utf-8')\nprint(tw_tr.head(),tw_te.head(), re_tr.head())",
      "execution_count": 342,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "          0  1                                                  2\n0  TrainSen  0  likers get tbh ; dun really know u but i know ...\n1  TrainSen  0  ðŸ¤‘: we have fun times you're chill asf and you'...\n2  TrainSen  0  @ValaAfshar : be lucky 1 work hard 2 be positi...\n3  TrainSen  0  @STurkle texting actually separates people as ...\n4  TrainSen  0  EVERYONE come out to the girls bball game tomo...           0  1                                                  2\n0  TrainSen  0  Sometimes I wish I learned how to speak Spanis...\n1  TrainSen  0         PSV certainly had a difficult game tonight\n2  TrainSen  1  I know man ! In for my leg finishing next week...\n3  TrainSen  0  I feel strangely elated because im so used to ...\n4  TrainSen  1  Yay . School . If you could only see the joy o...    label                                            comment     author  \\\n0      0                                         NC and NH.  Trumpbart   \n1      0  You do know west teams play against west teams...  Shbshb906   \n2      0  They were underdogs earlier today, but since G...   Creepeth   \n3      0  This meme isn't funny none of the \"new york ni...  icebrotha   \n4      0                    I could use one of those tools.  cush2push   \n\n            subreddit  score  ups  downs     date          created_utc  \\\n0            politics      2   -1     -1  2016-10  2016-10-16 23:55:23   \n1                 nba     -4   -1     -1  2016-11  2016-11-01 00:24:10   \n2                 nfl      3    3      0  2016-09  2016-09-22 21:45:37   \n3  BlackPeopleTwitter     -8   -1     -1  2016-10  2016-10-18 21:03:47   \n4  MaddenUltimateTeam      6   -1     -1  2016-12  2016-12-30 17:00:13   \n\n                                      parent_comment  \n0  Yeah, I get that argument. At this point, I'd ...  \n1  The blazers and Mavericks (The wests 5 and 6 s...  \n2                            They're favored to win.  \n3                         deadass don't kill my buzz  \n4  Yep can confirm I saw the tool they use for th...  \n"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import pandas as pd\nre_tr = pd.read_csv(basepath + '/data/combinded/train-balanced-sarcasm.csv', encoding='utf-8')\nred_dat = re_tr[['label','comment']]\nred_dat['Sen_type'] = 'TrainSen'\nred_dat.head()\ncols = red_dat.columns.tolist()\ncols = cols[-1:] + cols[:-1]\nred_dat = red_dat[cols]\nred_dat = red_dat.sample(frac=.05)\nred_dat.reset_index(drop=True, inplace=True)\nred_te = red_dat.iloc[:len(red_dat)//20, :]\nred_va = red_dat.iloc[len(red_dat)//20:2*(len(red_dat)//20), :]\nred_tr = red_dat.iloc[2*(len(red_dat))//20:, :]\nprint(red_te.shape, red_tr.shape, red_va.shape)\nred_te.to_csv(basepath + '/data/reddit/test/Test_v1.txt', header=False, index=False, sep='\\t', encoding='utf-8')\nred_va.to_csv(basepath + '/data/reddit/Dev_v1.txt', header=False, index=False, sep='\\t', encoding='utf-8')\nred_tr.to_csv(basepath + '/data/reddit/train/Train_v1.txt', index=False ,sep='\\t', header=False, encoding='utf-8')\n",
      "execution_count": 341,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "(2527, 3) (45487, 3) (2527, 3)\n"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "twit_dat1 = tw_tr.merge(tw_te, how='outer')\ntwit_dat2 = twit_dat1.merge(tw_va, how='outer')\ntwit_dat2.columns = ['Sen_type', 'label','comment']\ntwit_dat2.head()",
      "execution_count": 343,
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sen_type</th>\n      <th>label</th>\n      <th>comment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TrainSen</td>\n      <td>0</td>\n      <td>likers get tbh ; dun really know u but i know ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>TrainSen</td>\n      <td>0</td>\n      <td>ðŸ¤‘: we have fun times you're chill asf and you'...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>TrainSen</td>\n      <td>0</td>\n      <td>@ValaAfshar : be lucky 1 work hard 2 be positi...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>TrainSen</td>\n      <td>0</td>\n      <td>@STurkle texting actually separates people as ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>TrainSen</td>\n      <td>0</td>\n      <td>EVERYONE come out to the girls bball game tomo...</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "   Sen_type  label                                            comment\n0  TrainSen      0  likers get tbh ; dun really know u but i know ...\n1  TrainSen      0  ðŸ¤‘: we have fun times you're chill asf and you'...\n2  TrainSen      0  @ValaAfshar : be lucky 1 work hard 2 be positi...\n3  TrainSen      0  @STurkle texting actually separates people as ...\n4  TrainSen      0  EVERYONE come out to the girls bball game tomo..."
          },
          "execution_count": 343,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "comb_dat = red_dat.merge(twit_dat2, how='outer')\ncomb_dat.shape",
      "execution_count": 344,
      "outputs": [
        {
          "data": {
            "text/plain": "(93622, 3)"
          },
          "execution_count": 344,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "comb_dat = comb_dat.sample(frac=1)\ncomb_dat.reset_index(drop=True, inplace=True)\nprint(comb_dat.head())\ncomb_te = comb_dat.iloc[:len(comb_dat)//20, :]\ncomb_va = comb_dat.iloc[len(comb_dat)//20:2*(len(comb_dat)//20), :]\ncomb_tr = comb_dat.iloc[2*(len(comb_dat)//20):, :]\n\nprint(comb_tr.shape, comb_te.shape, comb_va.shape)",
      "execution_count": 345,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "   Sen_type  label                                            comment\n0  TrainSen      0  @realDonaldTrump @CNN why is ur response alway...\n1  TrainSen      1  Yay its pouring rain and I have xc this mornin...\n2  TrainSen      1    Forever fucked up in the head. Thanks. #sarcasm\n3  TrainSen      1                          It's pretty widely known.\n4  TrainSen      0  . ur kids it's a free download i'ma post it so...\n(84260, 3) (4681, 3) (4681, 3)\n"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "comb_te.to_csv(basepath + '/data/combinded/test/Test_v1.txt', header=False, index=False, sep='\\t', encoding='utf-8')\ncomb_va.to_csv(basepath + '/data/combinded/Dev_v1.txt', header=False, index=False, sep='\\t', encoding='utf-8')\ncomb_tr.to_csv(basepath + '/data/combinded/train/Train_v1.txt', index=False ,sep='\\t', header=False, encoding='utf-8')",
      "execution_count": 346,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}